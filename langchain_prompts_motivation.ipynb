{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32b637a-1a60-466d-8d5b-9255a72f2986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb80fb63-7197-421c-9eb7-d3b7aed8a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"llmapslab\")\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import load_configs\n",
    "import importlib\n",
    "importlib.reload(load_configs)\n",
    "from load_configs import (\n",
    "    openai_api_key,\n",
    "    llama_api_key, ai21_api_key, \n",
    "    gemini_api_key\n",
    ")\n",
    "from openai_client import call_openai_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3bcc4-a66c-4ce9-8155-5b443f43acc4",
   "metadata": {},
   "source": [
    "### Standardizing message format through LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be836044-b747-4f78-a5f8-8a8a54efd78f",
   "metadata": {},
   "source": [
    "BaseMessage(Serializable) - > BaseMessage --> SystemMessage, AIMessage, HumanMessage, ChatMessage, FunctionMessage, ToolMessage\n",
    "                --> BaseMessageChunk --> SystemMessageChunk, AIMessageChunk, HumanMessageChunk, ChatMessageChunk, FunctionMessageChunk, ToolMessageChunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b39887-5e3f-4183-9e00-4cf813b2304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    SystemMessage, HumanMessage, AIMessage, ChatMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e057cef5-ef43-49b6-8900-0f94a84a0de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are AI assistant for one line answer. Your name is Lisa'\n",
      "{'content': 'You are AI assistant for one line answer. Your name is Lisa', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}\n",
      "\n",
      "content='What type of harware is used for quantum computing'\n"
     ]
    }
   ],
   "source": [
    "sysmsg = SystemMessage(\"You are AI assistant for one line answer. Your name is Lisa\")\n",
    "hmsg = HumanMessage(\"What type of harware is used for quantum computing\") \n",
    "print(sysmsg)\n",
    "print(sysmsg.dict()) ###-> they are serializable for saving converting to json etc. \n",
    "print()\n",
    "print(hmsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab0fa41-3d8b-4294-abcd-f3ccbbf758d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='hhhh', role='system')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_chatmsg = ChatMessage(role=\"system\", content=\"hhhh\")\n",
    "sys_chatmsg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354d6db-38fd-49fd-83d2-80052dff894e",
   "metadata": {},
   "source": [
    "FunctionMessage\n",
    "This represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.\n",
    "\n",
    "ToolMessage\n",
    "This represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool message types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "113bf0a6-26cd-4e86-b439-660da4e64a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content=\"I'd like to know the revenue for wireless mouse.\"), FunctionMessage(content='Calculate revenue for a specific product', name='calculate_revenue', arguments={'product_name': 'wireless mouse'}), AIMessage(content='Calculating the revenue for wireless mouse...')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import FunctionMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Define the function message\n",
    "function_message = FunctionMessage(\n",
    "    content=\"Calculate revenue for a specific product\",\n",
    "    name=\"calculate_revenue\",\n",
    "    arguments={\n",
    "        \"product_name\": \"wireless mouse\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the ChatTemplatePrompt with function message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"I'd like to know the revenue for wireless mouse.\"),\n",
    "    function_message,\n",
    "    AIMessage(content=\"Calculating the revenue for wireless mouse...\")\n",
    "])\n",
    "\n",
    "# Invoke the prompt with the necessary context (if any)\n",
    "prompt_value = chat_prompt.invoke({\n",
    "    \"product_name\": \"wireless mouse\"\n",
    "})\n",
    "\n",
    "print(prompt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c159c824-1566-41d9-b439-854814ccaba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant.'), HumanMessage(content=\"I'd like to know the revenue for wireless mouse.\"), ToolMessage(content='Retrieve the revenue data', name='calculate_revenue', tool_call_id='1', artifact=[1, 2, 3, 4], arguments={'product_name': 'wireless mouse'}), AIMessage(content='Calculating the revenue for wireless mouse...')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Define the tool message\n",
    "tool_message = ToolMessage(\n",
    "    tool_call_id=1,\n",
    "    content=\"Retrieve the revenue data\",\n",
    "    artifact=[1, 2, 3, 4],\n",
    "    name=\"calculate_revenue\",\n",
    "    arguments={\n",
    "        \"product_name\": \"wireless mouse\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the ChatTemplatePrompt with tool message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"I'd like to know the revenue for wireless mouse.\"),\n",
    "    tool_message,\n",
    "    AIMessage(content=\"Calculating the revenue for wireless mouse...\")\n",
    "])\n",
    "\n",
    "# Invoke the prompt with the necessary context (if any)\n",
    "prompt_value = chat_prompt.invoke({\n",
    "    \"product_name\": \"wireless mouse\"\n",
    "})\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7703c73-0289-48d4-8756-9fb67be2341c",
   "metadata": {},
   "source": [
    "### Langchain Prompt\n",
    "inserting variables inside the message so that user can only provide input variables  instead of full text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e229486-d6b8-49d4-86f0-db0119baa104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### python string template with some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf41b03-401b-4171-970d-aa32f18b203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_name = \"\"\n",
    "sysmsg_template_str = \"\"\"You are {bot_role} for one line answer. \n",
    "Your name is {bot_name}\"\"\"\n",
    "user_input = \"\"        \n",
    "hmsg_template_str = \"\"\"{user_input}\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfaa05-eb45-4bef-a0d6-8e27c16023cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Langchain templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f0e6ff-efcc-44bf-a716-8cad5fb2f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,    \n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cc93b-e912-4f07-85c5-20a0667b3674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### PromptTemplate is generic template - see the argument and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2c5566-a8a2-477f-9f20-e23a81baee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt = PromptTemplate.from_template(\n",
    "    sysmsg_template_str)\n",
    "str_prompt\n",
    "## identifies the input_variables from the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c26543-0b40-4092-981e-7f759cd00617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'input_variables': ['bot_name', 'bot_role'],\n",
       " 'optional_variables': [],\n",
       " 'input_types': {},\n",
       " 'output_parser': None,\n",
       " 'partial_variables': {},\n",
       " 'metadata': None,\n",
       " 'tags': None,\n",
       " 'template': 'You are {bot_role} for one line answer. \\nYour name is {bot_name}',\n",
       " 'template_format': 'f-string',\n",
       " 'validate_template': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974aa66-56c3-4921-afa2-92897fd16061",
   "metadata": {},
   "source": [
    "##### optional_variables are for MessagePlaceholder in ChatPromptTemplate. This should not be displayed here. I have submitted:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a22b18f-67cc-4428-8c40-18c24f0c6c7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "https://github.com/langchain-ai/langchain/issues/24884\n",
    "\n",
    "https://github.com/langchain-ai/langchain/pull/25017\n",
    "\n",
    "###### partial variables can be upfront defined so that user need not input them repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50a7c56-ca05-4b01-a73c-fe9cf051dc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['bot_name'], partial_variables={'bot_role': 'AI Assistant'}, template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt = PromptTemplate.from_template(\n",
    "    sysmsg_template_str, partial_variables={\n",
    "        \"bot_role\": \"AI Assistant\"\n",
    "    })\n",
    "str_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84a755-a715-40f8-a348-fee27122f689",
   "metadata": {},
   "source": [
    "##### Another way of defining partial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda7d197-9e50-4354-9890-801988fb92bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt = PromptTemplate.from_template(\n",
    "    sysmsg_template_str)\n",
    "str_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50238c2-cd16-4972-acd8-ea114fad9615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['bot_name'], partial_variables={'bot_role': 'AI Assistant'}, template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt.partial(bot_role= \"AI Assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651cce07-5d84-4678-abb1-956fa0f1869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Error: 'bot_role'\n"
     ]
    }
   ],
   "source": [
    "### parital variable is not working with PromptTemplate\n",
    "try:\n",
    "    str_prompt.format(bot_name=\"Lisa\")\n",
    "except Exception as e:\n",
    "    print('Key Error:', e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb54272-a7c6-4348-85f8-46712f500315",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### parital variable is not working with PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21ba88-593c-466b-a3d2-9bd40369f345",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[28], line 1\n",
    "----> 1 str_prompt.format(bot_name=\"Lisa\")\n",
    "\n",
    "File /mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/langchain_core/prompts/prompt.py:179, in PromptTemplate.format(self, **kwargs)\n",
    "    170 \"\"\"Format the prompt with the inputs.\n",
    "    171 \n",
    "    172 Args:\n",
    "   (...)\n",
    "    176     A formatted string.\n",
    "    177 \"\"\"\n",
    "    178 kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "--> 179 return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n",
    "\n",
    "File ~/anaconda3/lib/python3.9/string.py:161, in Formatter.format(self, format_string, *args, **kwargs)\n",
    "    160 def format(self, format_string, /, *args, **kwargs):\n",
    "--> 161     return self.vformat(format_string, args, kwargs)\n",
    "\n",
    "File /mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/langchain_core/utils/formatting.py:31, in StrictFormatter.vformat(self, format_string, args, kwargs)\n",
    "     26 if len(args) > 0:\n",
    "     27     raise ValueError(\n",
    "     28         \"No arguments should be provided, \"\n",
    "     29         \"everything should be passed as keyword arguments.\"\n",
    "     30     )\n",
    "---> 31 return super().vformat(format_string, args, kwargs)\n",
    "\n",
    "File ~/anaconda3/lib/python3.9/string.py:165, in Formatter.vformat(self, format_string, args, kwargs)\n",
    "    163 def vformat(self, format_string, args, kwargs):\n",
    "    164     used_args = set()\n",
    "--> 165     result, _ = self._vformat(format_string, args, kwargs, used_args, 2)\n",
    "    166     self.check_unused_args(used_args, args, kwargs)\n",
    "    167     return result\n",
    "\n",
    "File ~/anaconda3/lib/python3.9/string.py:205, in Formatter._vformat(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\n",
    "    201     auto_arg_index = False\n",
    "    203 # given the field_name, find the object it references\n",
    "    204 #  and the argument it came from\n",
    "--> 205 obj, arg_used = self.get_field(field_name, args, kwargs)\n",
    "    206 used_args.add(arg_used)\n",
    "    208 # do any conversion on the resulting object\n",
    "\n",
    "File ~/anaconda3/lib/python3.9/string.py:270, in Formatter.get_field(self, field_name, args, kwargs)\n",
    "    267 def get_field(self, field_name, args, kwargs):\n",
    "    268     first, rest = _string.formatter_field_name_split(field_name)\n",
    "--> 270     obj = self.get_value(first, args, kwargs)\n",
    "    272     # loop through the rest of the field_name, doing\n",
    "    273     #  getattr or getitem as needed\n",
    "    274     for is_attr, i in rest:\n",
    "\n",
    "File ~/anaconda3/lib/python3.9/string.py:227, in Formatter.get_value(self, key, args, kwargs)\n",
    "    225     return args[key]\n",
    "    226 else:\n",
    "--> 227     return kwargs[key]\n",
    "\n",
    "KeyError: 'bot_role'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2a02d-6009-4f6a-94cd-effb70ff857d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### making the variable not mandatory and also hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123adfb6-d0bc-4aeb-b841-8659b56eb1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['bot_name'], partial_variables={'bot_role': ''}, template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt.partial(bot_role= \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f6839-4626-4534-aab2-aa3f66ca90a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### invoke produces a StringPromptValue class whereas format gives the text from the prompt with variable filled in  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1bf23f-6019-42a0-8e8a-f16d1ded5168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are assistant for one line answer. \\nYour name is Lisa')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_prompt.invoke(input={\"bot_name\": \"Lisa\", \"bot_role\": \"assistant\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9001067-5d28-4fa6-b2d1-14404ff935ed",
   "metadata": {},
   "source": [
    "####  Message Specific Templates for system, human, ai etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0247d049-6b0d-46bc-81e3-f64498959b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', role='Jedi')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"May the {subject} be with you\"\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"Jedi\", template=prompt\n",
    ")\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22bffcca-e5db-4a47-9c5a-7b615a8f0b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='You are ai assistant for one line answer. \\nYour name is Lisa')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sysmsg_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    sysmsg_template_str)\n",
    "print(sysmsg_prompt)\n",
    "sysmsg_prompt.format(bot_role=\"ai assistant\", bot_name=\"Lisa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03f3c113-8978-4b08-b6a2-6e18cde098a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usrmsg_prompt = HumanMessagePromptTemplate.from_template(hmsg_template_str)\n",
    "usrmsg_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c82c6f61-fcdb-4e14-ad2c-4da0eef42ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='what is your name')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usrmsg_prompt.format(user_input=\"what is your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5189846-302f-40de-95ad-34aae1777ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='what is your name')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usrmsg_prompt.format(**{\"user_input\": \"what is your name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7fb85e7-5df1-4cf8-9322-8c31a2105f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is your name')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usrmsg_prompt.format_messages(**{\"user_input\": \"what is your name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d0b3a-4b3b-4d35-a259-9c1ae3c19808",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ChatPromptTemplate is for ChatModels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dc01d-366a-47a2-8a17-d41332179be7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### RAW usage\n",
    "- instead of string, the input are a List[Tuple('role', 'content')]\n",
    "- Callable, initializttion can be done wiithout .from_template method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5169bc89-1fe6-4ede-a1f8-0ec1b978685e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['bot_name', 'user_input'], partial_variables={'bot_role': 'AI Assistant'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate([\n",
    "    ('system', sysmsg_template_str),\n",
    "    ('human', hmsg_template_str),\n",
    "],\n",
    "    partial_variables={\n",
    "        \"bot_role\": \"AI Assistant\"\n",
    "    }\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144deee-5fbb-4ca4-b7ae-633b0a90dee8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Lets reuse the message template instead of tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e14e3b64-4165-47de-afe7-5f9a865a76f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], partial_variables={'bot_role': 'AI Assistant', 'bot_name': 'Lisa'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate([\n",
    "    sysmsg_prompt,\n",
    "    usrmsg_prompt,\n",
    "],\n",
    "    partial_variables={\n",
    "        \"bot_role\": \"AI Assistant\",\n",
    "        'bot_name': 'Lisa'\n",
    "    }\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83f527b-48fd-4cf1-a005-adf39aeef8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are AI Assistant for one line answer. \\nYour name is Lisa'), HumanMessage(content='hi how r u ')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.invoke({'user_input':'hi how r u '})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692565c0-f151-48e2-a5c3-a03214c1a2d1",
   "metadata": {},
   "source": [
    "##### MessagePlaceHolder and optional_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0dd040b-ae5f-4097-a889-56291b296566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key error: 'history'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI assistant.'),\n",
       " HumanMessage(content='Hello!')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "placeholder_prompt = MessagesPlaceholder(\"history\")\n",
    "try:\n",
    "    placeholder_prompt.format_messages() # raises KeyError\n",
    "except Exception as e:\n",
    "    print('key error:', e)\n",
    "placeholder_prompt = MessagesPlaceholder(\"history\", optional=True)\n",
    "placeholder_prompt.format_messages() # returns empty list []\n",
    "\n",
    "placeholder_prompt.format_messages(\n",
    "    history=[\n",
    "        (\"system\", \"You are an AI assistant.\"),\n",
    "        (\"human\", \"Hello!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f006edf-9f58-4a7e-a3e1-adbbc570c110",
   "metadata": {},
   "source": [
    "##### use MessagePlaceholder in ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "390ab9e0-2d6c-4f44-8599-c156bb461bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], optional_variables=['history'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'bot_role': 'AI Assistant', 'bot_name': 'Lisa', 'history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')), MessagesPlaceholder(variable_name='history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate([\n",
    "    sysmsg_prompt,\n",
    "    MessagesPlaceholder(\"history\", optional=True),\n",
    "    usrmsg_prompt,\n",
    "    \n",
    "],\n",
    "    partial_variables={\n",
    "        \"bot_role\": \"AI Assistant\",\n",
    "        'bot_name': 'Lisa'\n",
    "    }\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b338979-eca5-4fe9-9500-28450e8cdcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are AI Assistant for one line answer. \\nYour name is Lisa'), HumanMessage(content='hi how r u ')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.invoke({'user_input':'hi how r u '})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6617d11-754c-427e-a9b3-56e6f5b3be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are AI Assistant for one line answer. \\nYour name is Lisa'), HumanMessage(content=\"what's 5 + 2\"), AIMessage(content='5 + 2 is 7'), HumanMessage(content='now multiply that by 4')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value = chat_prompt.invoke({\n",
    "                   \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
    "                   \"user_input\": \"now multiply that by 4\"\n",
    "               })\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a3ce073-01ee-403f-884b-d20ab2325dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are AI Assistant for one line answer. \\nYour name is Lisa'),\n",
       " HumanMessage(content=\"what's 5 + 2\"),\n",
       " AIMessage(content='5 + 2 is 7'),\n",
       " HumanMessage(content='now multiply that by 4')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe77b0-40b6-4540-9836-2003bbe00228",
   "metadata": {},
   "source": [
    "### Insert Exxample in Prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acff08-f3c0-4fc5-b1f6-b78e3b1ab1af",
   "metadata": {},
   "source": [
    "#### We can build a custom example selector class , see the basic logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffeec3ff-8001-49a5-a46d-05c2c92f6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors.base import BaseExampleSelector\n",
    "# help(BaseExampleSelector)\n",
    "class CustomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        # This assumes knowledge that part of the input will be a 'text' key\n",
    "        new_word = input_variables[\"input\"]\n",
    "        new_word_length = len(new_word)\n",
    "        # Initialize variables to store the best match and its length difference\n",
    "        best_match = None\n",
    "        smallest_diff = float(\"inf\")\n",
    "        # Iterate through each example\n",
    "        for example in self.examples:\n",
    "            # Calculate the length difference with the first word of the example\n",
    "            current_diff = abs(len(example[\"input\"]) - new_word_length)\n",
    "            # Update the best match if the current one is closer in length\n",
    "            if current_diff < smallest_diff:\n",
    "                smallest_diff = current_diff\n",
    "                best_match = example\n",
    "        return [best_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9758043-b919-4991-a932-ab2e20b8b94c",
   "metadata": {},
   "source": [
    "#### LangChain semactic similary example selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b6d40fe-83bb-4c7f-91d9-009e407e66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.example_selectors import (\n",
    "    SemanticSimilarityExampleSelector, \n",
    "    MaxMarginalRelevanceExampleSelector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79e8d961-ddca-41eb-9617-938fc8067728",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [ ### have to be in list of dictionary format\n",
    "     ### the keys are  the input_variables or partial_variables of the template\n",
    "    {\"example_user_input\": \"give me output in streaming format\",\n",
    "     \"example_answer\": '[\"My\", \"name\", \"is\", \"Lisa\"]' },\n",
    "    {\"example_user_input\": \"give me output in numerical format\",\n",
    "     \"example_answer\": '2000' }\n",
    "    \n",
    "]\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,    \n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),    \n",
    "    Chroma, # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    k=1,  # The number of examples to produce.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25f9904b-ff21-45a2-ae89-ed68fecb486c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'example_answer': '[\"My\", \"name\", \"is\", \"Lisa\"]',\n",
       "  'example_user_input': 'give me output in streaming format'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector.select_examples({\"example_user_input\": \"stream a one line story\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "479afbda-678b-4906-8834-171f273d1973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'example_answer': '2000',\n",
       "  'example_user_input': 'give me output in numerical format'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector.select_examples({\"example_user_input\": \"what is 48 percent of 85 million\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942e205-65b2-41bd-8c91-a6e46a3258a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FewShowPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bc69ed6-28c4-4937-a0b9-e2c2c8ecb5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['user_input'], example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f824f552df0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=PromptTemplate(input_variables=['example_answer', 'example_user_input'], template='example_user_input: {example_user_input}\\nexample_answer: {example_answer}'), suffix='Input: {user_input}\\nOutput:', prefix='Consult with the examples before answer:')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    # input_variables=[\"input\", \"output\"],\n",
    "    template=\"example_user_input: {example_user_input}\\nexample_answer: {example_answer}\",\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Consult with the examples before answer:\",\n",
    "    suffix=\"Input: {user_input}\\nOutput:\",\n",
    "    # input_variables=[\"adjective\"],\n",
    ")\n",
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2e628d9-a60d-4916-8b25-1f4c7ab76d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consult with the examples before answer:\\n\\nexample_user_input: give me output in streaming format\\nexample_answer: [\"My\", \"name\", \"is\", \"Lisa\"]\\n\\nInput: tell me a poem in stream mode\\nOutput:'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_prompt.format(user_input=\"tell me a poem in stream mode\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72ebff23-7373-4fa4-82b8-2e1d20fbc1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consult with the examples before answer:\\n\\nexample_user_input: give me output in numerical format\\nexample_answer: 2000\\n\\nInput: 35 perccent discount on one lakh\\nOutput:'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_prompt.format(user_input=\"35 perccent discount on one lakh\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b61de-964f-401f-ab47-6f78d8927457",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b7cd06e-4dec-44bf-8f80-6585a32c6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ee7c7-ec12-4806-9043-88052ec86e5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Wrong way of using the example_prompt in FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c65927d5-ee60-40fd-b031-4850ca791055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'example_user_input': 'give me output in streaming format', 'example_answer': '[\"My\", \"name\", \"is\", \"Lisa\"]'}, {'example_user_input': 'give me output in numerical format', 'example_answer': '2000'}], example_prompt=ChatPromptTemplate(input_variables=['user_input'], optional_variables=['history'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'bot_role': 'AI Assistant', 'bot_name': 'Lisa', 'history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name', 'bot_role'], template='You are {bot_role} for one line answer. \\nYour name is {bot_name}')), MessagesPlaceholder(variable_name='history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_chat_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=chat_prompt, ##this prompt is to format the examples and not for users interaction with model\n",
    "    examples=examples\n",
    ")\n",
    "fewshot_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffef0ea6-f242-4611-a9ac-149465c8971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier we constructed the chat_prompt \n",
    "# chat_prompt = ChatPromptTemplate([\n",
    "#     sysmsg_prompt,\n",
    "#     MessagesPlaceholder(\"history\", optional=True),\n",
    "#     usrmsg_prompt,\n",
    "    \n",
    "# ],\n",
    "#     partial_variables={\n",
    "#         \"bot_role\": \"AI Assistant\",\n",
    "#         'bot_name': 'Lisa'\n",
    "#     }\n",
    "# )\n",
    "# chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8331e55c-bd86-4977-b94e-140bba2c033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key error: 'user_input'\n"
     ]
    }
   ],
   "source": [
    "### can not use it when ther are input variables in the \n",
    "try:\n",
    "    fewshot_chat_prompt.format(user_input=\"tell me a story\")\n",
    "except Exception as e:\n",
    "    print('key error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6bd44-60d4-4b35-9daf-ec8c82434195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### correct usage of  FewShotChatMessagePromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28130f29-569c-4890-a37b-faf5968d9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['example_answer', 'exapmple_user_input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['exapmple_user_input'], template='{exapmple_user_input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_answer'], template='{example_answer}'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_for_example = ChatPromptTemplate(\n",
    "    [ \n",
    "        HumanMessagePromptTemplate.from_template(\"{exapmple_user_input}\"), ### equivallent (\"human\", \"exapmple_user_input\")\n",
    "        AIMessagePromptTemplate.from_template(\"{example_answer}\"),        \n",
    "    ],\n",
    "    # input_variables=['example_answer', 'exapmple_user_input']\n",
    ")\n",
    "prompt_for_example    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91bba276-74dd-4cb8-bc4b-cb1399ab8dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['example_answer', 'example_user_input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_user_input'], template='{example_user_input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_answer'], template='{example_answer}'))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_for_example = ChatPromptTemplate(\n",
    "    [ \n",
    "        (\"human\", \"{example_user_input}\"),\n",
    "        (\"ai\", \"{example_answer}\")\n",
    "            ]\n",
    ")\n",
    "prompt_for_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfc0b2e7-0f0a-418c-99ab-f6e24df68b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'example_user_input': 'give me output in streaming format', 'example_answer': '[\"My\", \"name\", \"is\", \"Lisa\"]'}, {'example_user_input': 'give me output in numerical format', 'example_answer': '2000'}], example_prompt=ChatPromptTemplate(input_variables=['example_answer', 'example_user_input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_user_input'], template='{example_user_input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_answer'], template='{example_answer}'))]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshow_chat_message_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=prompt_for_example,\n",
    "    examples=examples\n",
    ")\n",
    "fewshow_chat_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1292f7c-b760-4bde-ba0f-8aac60843c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: give me output in streaming format\\nAI: [\"My\", \"name\", \"is\", \"Lisa\"]\\nHuman: give me output in numerical format\\nAI: 2000'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshow_chat_message_prompt.format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb49ee-b563-4925-a113-3ec3c491e2c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Use the fewshow example in the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d53343c-8362-411b-9be6-5a288866e226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], messages=[SystemMessage(content='Consult the following Examples:'), FewShotChatMessagePromptTemplate(examples=[{'example_user_input': 'give me output in streaming format', 'example_answer': '[\"My\", \"name\", \"is\", \"Lisa\"]'}, {'example_user_input': 'give me output in numerical format', 'example_answer': '2000'}], example_prompt=ChatPromptTemplate(input_variables=['example_answer', 'example_user_input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_user_input'], template='{example_user_input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_answer'], template='{example_answer}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chat_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessage(content=\"Consult the following Examples:\"),\n",
    "        fewshow_chat_message_prompt,\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "final_chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a91fa3af-705b-46a4-9c6f-d5b64687d215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: Consult the following Examples:\\nHuman: give me output in streaming format\\nAI: [\"My\", \"name\", \"is\", \"Lisa\"]\\nHuman: give me output in numerical format\\nAI: 2000\\nHuman: how is the weather'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chat_prompt.format(user_input='how is the weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41791e6e-61b0-4172-9a7b-e7b0d4fe4670",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "####  Example Selector dynamically in ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08b592e9-ca36-4cee-b58e-2ca4e3e55128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f824f552df0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We have already created a example_selector previously but this \n",
    "### is another way to do it\n",
    "# to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# example_selector = SemanticSimilarityExampleSelector(\n",
    "#     vectorstore=vectorstore,\n",
    "#     k=2,\n",
    "# )\n",
    "example_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d79fa6d-5e28-451f-b647-b4f66740fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f824f552df0>, k=1, example_keys=None, input_keys=None, vectorstore_kwargs=None), example_prompt=ChatPromptTemplate(input_variables=['example_answer', 'example_user_input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_user_input'], template='{example_user_input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['example_answer'], template='{example_answer}'))]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_fewshow_chat_message_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # input_variables=['example_user_input'],\n",
    "    example_prompt=prompt_for_example,\n",
    "    example_selector=example_selector\n",
    ")\n",
    "dynamic_fewshow_chat_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7378bf1e-c6a4-4dc1-9ad9-266c61f944e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: give me output in numerical format\\nAI: 2000'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_fewshow_chat_message_prompt.format(user_input='what is the temparature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64989e50-91ff-44d9-99a6-f245aefaffce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: give me output in streaming format\\nAI: [\"My\", \"name\", \"is\", \"Lisa\"]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_fewshow_chat_message_prompt.format(user_input='who was the king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ba060de-e415-4879-96b1-7b1e5566699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a96c9014-1f39-4ee8-be97-9a1c73a4307c'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector.add_example({\n",
    "    \"example_user_input\": \"sing me a song\",\n",
    "    \"example_answer\": \"Chord A , Chord C, ....Chord D\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13b84233-f4e0-4bc8-9809-9ebfe3ec1614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: sing me a song\\nAI: Chord A , Chord C, ....Chord D'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_fewshow_chat_message_prompt.format(user_input='can you sing jingle bell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c66cfd-9afd-4870-81f0-6683fd759e75",
   "metadata": {},
   "source": [
    "### Partial Prompt - from function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96dc3f21-c136-4e31-8ed6-999661e00368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d50a61ab-99a4-44fe-8c30-c8bb6455abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 08/04/2024, 18:49:05\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d7e2f39-d033-4e39-8d9b-5f39da9c9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 08/04/2024, 18:49:20\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "print(prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bd4bf-c7da-4138-b0a2-0c3c236001ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e94b6edf-f6c8-48e2-815b-ef2fd0a237df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = (\n",
    "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    + \", make it funny\"\n",
    "    + \"\\n\\nand in {language}\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfa8a30e-8d7a-4f74-82ca-000865d4fefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a nice pirate'),\n",
       " HumanMessage(content='hi'),\n",
       " AIMessage(content='what?'),\n",
       " HumanMessage(content='i said hi')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = SystemMessage(content=\"You are a nice pirate\")\n",
    "new_prompt = (\n",
    "    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
    ")\n",
    "new_prompt.format_messages(input=\"i said hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910c06d-9c57-4e9f-b083-4a74661fbeb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PipeinePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72a0317e-5083-47c4-8fd6-f59e455632ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.pipeline import PipelinePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "923f8be8-7720-4d77-9257-da91e773c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"\n",
    "{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start_action}\n",
    "\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "908fb60e-3655-4805-97eb-f4201d39d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_prompt = PromptTemplate.from_template(\n",
    "    \"introduction for {subject}\")\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"follow the example:\n",
    "    Question: what is  the application of Low Rank matrix in {example_subject}\n",
    "    Answer: \" efficient calculation and reduction of GPU RAM usage\"\"\"\n",
    "    )\n",
    "start_action_prompt = PromptTemplate.from_template(\n",
    "    \"You need to now initiate {action_item}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "656a99a2-1294-4b06-84eb-e8d6a5f1f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start_action\", start_action_prompt)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed93cbc0-0b37-4b37-b9c2-081570856f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelinePromptTemplate(input_variables=['subject', 'example_subject', 'action_item'], final_prompt=PromptTemplate(input_variables=['example', 'introduction', 'start_action'], template='\\n{introduction}\\n\\n{example}\\n\\n{start_action}\\n'), pipeline_prompts=[('introduction', PromptTemplate(input_variables=['subject'], template='introduction for {subject}')), ('example', PromptTemplate(input_variables=['example_subject'], template='follow the example:\\n    Question: what is  the application of Low Rank matrix in {example_subject}\\n    Answer: \" efficient calculation and reduction of GPU RAM usage')), ('start_action', PromptTemplate(input_variables=['action_item'], template='You need to now initiate {action_item}'))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_template = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt,\n",
    "    pipeline_prompts=prompt_list\n",
    ")\n",
    "final_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bfb891de-b160-4c94-a328-9af8df285b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nintroduction for Artificial Intelligence\\n\\nfollow the example:\\n    Question: what is  the application of Low Rank matrix in Machine Learning\\n    Answer: \" efficient calculation and reduction of GPU RAM usage\\n\\nYou need to now initiate draw matrix\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_template.format(\n",
    "    subject=\"Artificial Intelligence\",\n",
    "    example_subject=\"Machine Learning\",\n",
    "    action_item=\"draw matrix\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
