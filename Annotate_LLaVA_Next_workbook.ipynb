{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio\n",
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/miniconda3/envs/final_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Chat templates should be in a 'chat_template.json' file but found key='chat_template' in the processor's config. Make sure to move your template to its own file.\n",
      "You are using a model of type llava_next to instantiate a model of type llava_next_video. This is not supported for all configurations of models and can yield errors.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_path = '/home/manish/final/LLaVA-NeXT-Video-7B-DPO-hf/models--llava-hf--LLaVA-NeXT-Video-7B-DPO-hf/snapshots/d30328aaf846b128f919353e18bd0cfe5fcdfd10'\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(model_path)\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "import torch\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Set random seed\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Model Loading for Logic Extraction\n",
    "class ModelLoader:\n",
    "    _model = None\n",
    "    _tokenizer = None\n",
    "    _pipe = None\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, model_path):\n",
    "        if cls._model is None or cls._tokenizer is None:\n",
    "            cls._model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"cuda\",\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            cls._tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            cls._pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=cls._model,\n",
    "                tokenizer=cls._tokenizer,\n",
    "            )\n",
    "        return cls._pipe\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.1,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "class LLMHelper:\n",
    "    def __init__(self, pipeline):\n",
    "        self.chatbot = pipeline\n",
    "\n",
    "    def generate_logic(self, llm_output: str):\n",
    "        prompt = f\"\"\"\n",
    "        Provide the response in json string for the below keys and context based on the description: '{llm_output}'.\n",
    "        \n",
    "        Screen.interaction_yes: This field indicates whether there was an interaction of the person with a screen during the activity. A value of 1 means there was screen interaction (Yes), and a value of 0 means there was no screen interaction (No).\n",
    "        Hands.free: This field indicates whether the person's hands were free during the activity. A value of 1 means the person was not holding anything (Yes), indicating free hands. A value of 0 means the person was holding something (No), indicating the hands were not free.\n",
    "        Indoors: This field indicates whether the activity took place indoors. A value of 1 means the activity occurred inside a building or enclosed space (Yes), and a value of 0 means the activity took place outside (No).\n",
    "        Standing: This field indicates whether the person was standing during the activity. A value of 1 means the person was standing (Yes), and a value of 0 means the person was not standing (No).\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Please answer questions just based on this information: \" + llm_output},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        response = self.chatbot(messages, **generation_args)\n",
    "        generated_text = response[0]['generated_text']\n",
    "        # Extract JSON from the generated text\n",
    "        start_index = generated_text.find('{')\n",
    "        end_index = generated_text.rfind('}') + 1\n",
    "        json_str = generated_text[start_index:end_index]\n",
    "        return json_str\n",
    "\n",
    "class VideoAnalysis(BaseModel):\n",
    "    screen_interaction_yes: int\n",
    "    hands_free: int\n",
    "    indoors: int\n",
    "    standing: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm_output(cls, llm_output: str, generated_logic: str) -> 'VideoAnalysis':\n",
    "        # Parse the generated logic (assuming it's a JSON string)\n",
    "        logic_dict = json.loads(generated_logic)\n",
    "        \n",
    "        return cls(\n",
    "            screen_interaction_yes=logic_dict.get(\"Screen.interaction_yes\", 0),\n",
    "            hands_free=logic_dict.get(\"Hands.free\", 0),\n",
    "            indoors=logic_dict.get(\"Indoors\", 0),\n",
    "            standing=logic_dict.get(\"Standing\", 0)\n",
    "        )\n",
    "\n",
    "# Load the logic extraction model\n",
    "model_path = \"/home/manish/Super-Rapid-Annotator-Multimodal-Annotation-Tool/models/Phi3\"\n",
    "logic_pipeline = ModelLoader.load_model(model_path)\n",
    "llm_helper = LLMHelper(logic_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import av\n",
    "import os\n",
    "import json\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "    Returns:\n",
    "        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def process_video(video_file, question):\n",
    "    # Open video and sample frames\n",
    "    with av.open(video_file.name) as container: # Access file name from Gradio input\n",
    "        total_frames = container.streams.video[0].frames\n",
    "        indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "        video_clip = read_video_pyav(container, indices)\n",
    "\n",
    "    # Prepare conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"{question}\"},\n",
    "                {\"type\": \"video\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    # Prepare inputs for the model\n",
    "    input = processor([prompt], videos=[video_clip], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    generate_kwargs = {\"max_new_tokens\": 3000, \"do_sample\": False, \"top_p\": 0.9}\n",
    "    output = model.generate(**input, **generate_kwargs)\n",
    "    generated_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text.split(\"ASSISTANT: \", 1)[-1].strip()\n",
    "\n",
    "\n",
    "def process_videos(video_files, question):\n",
    "    \"\"\"Processes multiple videos and answers a single question for each.\"\"\"\n",
    "    answers = []\n",
    "    for video_file in video_files:\n",
    "        video_name = os.path.basename(video_file.name)\n",
    "        answer = process_video(video_file, question)\n",
    "        answers.append(f\"**Video: {video_name}**\\n{answer}\\n\")\n",
    "    return \"\\n---\\n\".join(answers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a9658b152670d36655.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a9658b152670d36655.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Gradio interface for multiple videos\n",
    "def gradio_interface(videos, indoors_outdoors, standing_sitting, hands_free, interacting_screen):\n",
    "    question = \"For each question, analyze the given video carefully and base your answers on the observations made.\"\n",
    "    if indoors_outdoors:\n",
    "        question += \"Consider the broader environmental context shown in the video’s background. Are there signs of an open-air space, like greenery, structures, or people passing by? If so, it’s an outdoor setting. If the setting looks confined with furniture, walls, or home decorations, it’s an indoor environment.\"\n",
    "    if standing_sitting:\n",
    "        question += \"Evaluate the subject’s body posture and movement within the video. Are they standing upright with both feet planted firmly on the ground? If so, they are standing.\"\n",
    "    if hands_free:\n",
    "        question += \"Examine the subject’s right and left hands in the video to check if they are holding anything like a microphone, book, paper(White color), object, or any electronic device, try segmentations and decide if the hands are free or not.\"\n",
    "    if interacting_screen:\n",
    "        question += \"Assess the surroundings behind the subject in the video. Do they seem to interact with any visible screens, such as laptops, TVs, or digital billboards? If yes, then they are interacting with a screen. If not, they are not interacting with a screen.\"\n",
    "    question_prefix = \"By taking these factors into account when watching the video, please answer the questions accurately.\"\n",
    "    question = question + question_prefix \n",
    "    answers = process_videos(videos, question)\n",
    "    return answers\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload Videos\", file_count=\"multiple\"),\n",
    "        gr.Checkbox(label=\"Indoors or Outdoors\", value=False),\n",
    "        gr.Checkbox(label=\"Standing or Sitting\", value=False),\n",
    "        gr.Checkbox(label=\"Hands Free or Not\", value=False),\n",
    "        gr.Checkbox(label=\"Interacting with Screen\", value=False),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Answers\"),\n",
    "    title=\"Video Question Answering\",\n",
    "    description=\"Upload multiple videos and select questions to get answers.\"\n",
    ")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
