{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5XjiD4DIXf2yS/Vfc/Qr7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq transformers\n",
        "!pip install -Uq bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvjKL9VlB2VW",
        "outputId": "e63d9824-c137-44ef-be80-661bf9ed89b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    TimmWrapperImageProcessor,\n",
        "    TimmWrapperForImageClassification,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from transformers.image_utils import load_image\n",
        "import torch"
      ],
      "metadata": {
        "id": "m84dc6UiB9V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "checkpoint = \"timm/vit_base_patch16_224.augreg2_in21k_ft_in1k\"\n",
        "\n",
        "model = TimmWrapperForImageClassification.from_pretrained(checkpoint).to(\"cuda\")\n",
        "model_8bit = TimmWrapperForImageClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    quantization_config=quantization_config,\n",
        "    low_cpu_mem_usage=True,\n",
        ")"
      ],
      "metadata": {
        "id": "p2KWzbWIDVv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/timm/cat.jpg\"\n",
        "image = load_image(image_url)\n",
        "\n",
        "image_processor = TimmWrapperImageProcessor.from_pretrained(checkpoint)\n",
        "inputs = image_processor(image)"
      ],
      "metadata": {
        "id": "AbHBWzJ8Mvw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model):\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    top5_probabilities, top5_class_indices = torch.topk(logits.softmax(dim=1) * 100, k=5)\n",
        "\n",
        "    id2label = model.config.id2label\n",
        "\n",
        "    for idx, prob in zip(top5_class_indices[0], top5_probabilities[0]):\n",
        "        print(f\"Label: {id2label[idx.item()] :20} Score: {prob/100 :0.2f}%\")"
      ],
      "metadata": {
        "id": "Bal5EbqnM4Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI4FadNcMyHP",
        "outputId": "d1592a8f-b8b0-4c20-966b-00393ddf4570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: remote control, remote Score: 0.35%\n",
            "Label: tabby, tabby cat     Score: 0.27%\n",
            "Label: Egyptian cat         Score: 0.13%\n",
            "Label: tiger cat            Score: 0.11%\n",
            "Label: rule, ruler          Score: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference(model_8bit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtsdHRVXLrfu",
        "outputId": "9bc4a59d-bcdc-4f73-8884-401358583d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: remote control, remote Score: 0.33%\n",
            "Label: tabby, tabby cat     Score: 0.29%\n",
            "Label: Egyptian cat         Score: 0.13%\n",
            "Label: tiger cat            Score: 0.11%\n",
            "Label: rule, ruler          Score: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare memory footprints\n",
        "original_footprint = model.get_memory_footprint()\n",
        "quantized_footprint = model_8bit.get_memory_footprint()\n",
        "\n",
        "print(f\"Memory footprint of the original model: {original_footprint / 1e6:.2f} MB\")\n",
        "print(f\"Memory footprint of the quantized model: {quantized_footprint / 1e6:.2f} MB\")\n",
        "print(\n",
        "    f\"Reduction in memory usage: \"\n",
        "    f\"{(original_footprint - quantized_footprint) / original_footprint * 100:.2f}%\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abFs3JFPLmp1",
        "outputId": "397d66d6-7e44-421a-d3a1-83c5c6e8fff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint of the original model: 346.27 MB\n",
            "Memory footprint of the quantized model: 88.20 MB\n",
            "Reduction in memory usage: 74.53%\n"
          ]
        }
      ]
    }
  ]
}