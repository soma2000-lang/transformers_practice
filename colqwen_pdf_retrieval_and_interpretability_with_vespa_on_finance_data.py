# -*- coding: utf-8 -*-
"""ColQwen_pdf_retrieval_and_interpretability_with_Vespa_on_finance_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsFtG_4GDRHfJ0-7CIouNkqny471xHnD
"""

!pip install -q transformers --upgrade
!pip install -q "colpali-engine>=0.3.2,<0.4.0" --upgrade
!apt-get install poppler-utils
!pip install -q pdf2image pypdf
!pip install -q openai
!pip install -q qwen_vl_utils
!pip install -q pyvespa vespacli

from colpali_engine.models import ColQwen2, ColQwen2Processor
from colpali_engine.interpretability import (
    get_similarity_maps_from_embeddings,
    plot_all_similarity_maps,
    plot_similarity_map,
)

import numpy as np
import torch
from torch.utils.data import DataLoader

from tqdm import tqdm
from PIL import Image
from pdf2image import convert_from_path
import base64
import io
from io import BytesIO
from IPython.display import display, HTML
import requests
from pypdf import PdfReader
import os
import json

from vespa.deployment import VespaCloud
from vespa.application import Vespa
from vespa.package import ApplicationPackage, Schema, Document, Field, FieldSet, HNSW
from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking
from vespa.io import VespaResponse, VespaQueryResponse

# We will be working with these two pdfs
pdf_lists = [
    {"header": "Tesla finance", "url": "https://ir.tesla.com/_flysystem/s3/sec/000162828024043432/tsla-20241023-gen.pdf"},
    {"header": "Basic understanding of company finance", "url": "https://www.pwc.com/jm/en/research-publications/pdf/basic-understanding-of-a-companys-financials.pdf"}
]

model = ColQwen2.from_pretrained(
        "vidore/colqwen2-v0.1",
        torch_dtype=torch.bfloat16, # switch to float16 if gpu does not support it
        device_map="auto",
    )
processor = ColQwen2Processor.from_pretrained("vidore/colqwen2-v0.1")
model = model.eval()

# Utility function for downloading PDF from URL
def download_pdf(url):
    """returns: BytesIO: In-memory file object containing the PDF content."""
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return BytesIO(response.content)
    except Exception as e:
        raise Exception(f"Failed to download PDF: Status code {e}")


# Encodes images in base64
def encode_images_base64(images):
    """returns: list of str: Base64-encoded strings for each image."""
    base64_images = []
    for image in images:
        buffered = BytesIO()
        image.save(buffered, format="JPEG")
        base64_images.append(base64.b64encode(buffered.getvalue()).decode('utf-8'))
    return base64_images


# Processes a single PDF to images of each page
def get_pdf_images(pdf_url):
    pdf_file = download_pdf(pdf_url)
    pdf_file.seek(0)  # Reset file pointer for image conversion

    # gets images of each page from the PDF
    temp_file_path = "test.pdf"
    with open(temp_file_path, "wb") as f:
        f.write(pdf_file.read())
    pdf_images = convert_from_path(temp_file_path)

    return pdf_images


# Converts images to embeddings using a model and DataLoader
def generate_image_embeddings(images, model, processor, batch_size=4):  # adjust batch_size according to vram
    embeddings = []
    dataloader = DataLoader(
        images, batch_size=batch_size, shuffle=False,
        collate_fn=lambda x: processor.process_images(x)
    )
    for batch in tqdm(dataloader):
        with torch.no_grad():
            batch = {k: v.to(model.device) for k, v in batch.items()}
            batch_embeddings = model(**batch).to("cpu")    # after the embeddings are generated, moving them to the CPU avoids clogging up GPU memory
            embeddings.extend(list(batch_embeddings))
    return embeddings

"""Here we will be processing all the pdfs in the pdf_list, download the pdfs, convert each page to images and generate embeddings of each page using colqwen model."""

for pdf in pdf_lists:
    header = pdf.get("header", "Unnamed Document")
    url = pdf["url"]
    print(f"Process for {header} pdf started")
    header = pdf.get("header", "Unnamed Document")
    try:
        pdf_page_images = get_pdf_images(url)
        pdf["images"] = pdf_page_images
        pdf_embeddings = generate_image_embeddings(pdf_page_images, model, processor, batch_size=4)
        pdf["embeddings"] = pdf_embeddings
    except Exception as e:
        print(f"Error processing {header}: {e}")

# utility function to resize the images to a standard size
def resize_image(image, max_height=800):
    width, height = image.size
    if height > max_height:
        ratio = max_height / height
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        return image.resize((new_width, new_height))
    return image

"""Preparing tha data which will be inserted into Vespa vector database. The multivector embeddings were already generated above for the images. Here, we are doing slight modification -> binary quantization of vectors (i.e., float values will be converted to 0, 1), and images are converted to base64 encoding for easily storing the encoded string of the image to vespa.

Binary quantization: Say we have a vector [0.8, -0.5, 1.2, 0.3], upon binary quantization the vector is transformed to [1, 0, 1, 1]. Values less than equal to zero get 0, and more than 0 gets 1.
"""

vespa_feed = []
for pdf in pdf_lists:
    url = pdf["url"]
    title = pdf["header"]
    base64_images = encode_images_base64(resize_image(pdf["images"]))

    for page_number, (embedding, image, base64_img) in enumerate(zip(pdf["embeddings"], pdf["images"], base64_images)):
        embedding_dict = dict()
        for idx, patch_embedding in enumerate(embedding):
            binary_vector = (
                np.packbits(np.where(patch_embedding > 0, 1, 0))   # binary quantization of vectors
                .astype(np.int8)
                .tobytes()
                .hex()
            )
            embedding_dict[idx] = binary_vector
        page = {
            "id": hash(url + str(page_number)),
            "url": url,
            "title": title,
            "page_number": page_number,
            "image": base64_img,
            "embedding": embedding_dict,
        }
        vespa_feed.append(page)

len(vespa_feed)  # total numbers pages -> including all pdfs

vespa_feed[0].keys()

"""Here, we define the Vespa schema including fields, data types, indexing, and which fields will be available for search matching. This is pretty common in every database to define a schema before inserting data (like elastic search, weaviate, etc)."""

colpali_schema = Schema(
    name="finance_data_schema",   # name it according to your choice
    document=Document(
        fields=[
            Field(
                name="id",
                type="string",
                indexing=["summary", "index"],
                match=["word"]
            ),
            Field(
                name="url",
                type="string",
                indexing=["summary", "index"]
            ),
            Field(
                name="title",
                type="string",
                indexing=["summary", "index"],
                match=["text"],
                index="enable-bm25",
            ),
            Field(
                name="page_number",
                type="int",
                indexing=["summary", "attribute"]
            ),
            Field(
                name="image",
                type="raw",
                indexing=["summary"]
            ),
            Field(
                name="embedding",
                type="tensor<int8>(patch{}, v[16])",
                indexing=[
                    "attribute",
                    "index",
                ],  # adds HNSW index for candidate retrieval.
                ann=HNSW(
                    distance_metric="hamming",    # we are using hamming distance metric (hamming works the best with bits value, concept is to count the mismatch of bits numbers between two vectors)
                    max_links_per_node=32,
                    neighbors_to_explore_at_insert=400,
                ),
            )
        ]
    ),
    fieldsets=[
        FieldSet(name="default", fields=["title"])
    ]
)

"""Next, we define a ranking profile in Vespa that uses both Hamming distance (for binary similarity) and MaxSim (for float-based late interaction similarity) to rank documents. This profile enables a two-phase ranking: a first phase for initial filtering using binary similarity, and a second phase for re-ranking with continuous (float) similarity scores."""

input_query_tensors = []
MAX_QUERY_TERMS = 64
for i in range(MAX_QUERY_TERMS):
    input_query_tensors.append((f"query(rq{i})", "tensor<int8>(v[16])"))

input_query_tensors.append(("query(qt)", "tensor<float>(querytoken{}, v[128])"))
input_query_tensors.append(("query(qtb)", "tensor<int8>(querytoken{}, v[16])"))

colpali_retrieval_profile = RankProfile(
    name="retrieval-and-rerank",
    inputs=input_query_tensors,
    functions=[
        # Computes binary similarity using Hamming distance for query(qtb) and embedding.
        Function(
            name="max_sim_binary",
            expression="""
                sum(
                  reduce(
                    1/(1 + sum(
                        hamming(query(qtb), attribute(embedding)) ,v)
                    ),
                    max,
                    patch
                  ),
                  querytoken
                )
            """,
        ),
        # Computes similarity between the query(qt) tensor and the documentâ€™s embedding attribute.
        Function(
            name="max_sim",
            # query(qt) * unpack_bits(attribute(embedding)) : calculates the element-wise product of the query and document embeddings for similarity.
            # reduce(..., max, patch) : selects the maximum similarity value across patches
            # sum(..., querytoken) : aggregates the similarity score across all tokens.
            expression="""
                sum(
                    reduce(
                        sum(
                            query(qt) * unpack_bits(attribute(embedding)) , v
                        ),
                        max, patch
                    ),
                    querytoken
                )
            """,
        )
    ],
    first_phase=FirstPhaseRanking(expression="max_sim_binary"),   # first filtering of image patches using hamming distance (binary similarity)
    second_phase=SecondPhaseRanking(expression="max_sim", rerank_count=5),  # reranking of filtered top 5 image patches using maxsim
)
colpali_schema.add_rank_profile(colpali_retrieval_profile)

"""With the configured application, we can now deploy it to Vespa Cloud.

To deploy the application to Vespa Cloud we need to create an account (free trial works) and then a tenant in the https://console.vespa-cloud.com/.

For this step tenant_name and app_name are required which you can setup in Vespa cloud after creating your account. Create an application in Vespa Cloud after tenant is created, and paste the name here below. Since we are not giving key below, you need to login interactively from here.
"""

app_name = "colqwenpdfrag"   # name this according to your choice
vespa_application_package = ApplicationPackage(
    name=app_name, schema=[colpali_schema]
)


os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Replace with your tenant name from the Vespa Cloud Console
tenant_name = "antaripaproject"

vespa_cloud = VespaCloud(
    tenant=tenant_name,
    application=app_name,
    application_package=vespa_application_package,
)

app: Vespa = vespa_cloud.deploy()

"""Here, we are inserting the full vespa_feed of 70 rows that we prepared before in the database."""

async with app.asyncio(connections=1, timeout=180) as session:
    for page in tqdm(vespa_feed):
        response: VespaResponse = await session.feed_data_point(
            data_id=page["id"], fields=page, schema="finance_data_schema"
        )
        if not response.is_successful():
            print(response.json())

"""Time for testing the retrieval with a given query"""

queries = [
    "balance at 1 July 2017 for equity holders",
]

"""Generate the embeddings of the query using the same colqwen model"""

dataloader = DataLoader(
    queries,
    batch_size=1,
    shuffle=False,
    collate_fn=lambda x: processor.process_queries(x),
)
qs = []
for batch_query in dataloader:
    with torch.no_grad():
        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}
        embeddings_query = model(**batch_query)
        qs.extend(list(torch.unbind(embeddings_query.to("cpu"))))

"""Utility function to display the top retrieved results images in presentable format. Since we saved bse64 format image in vespa, first we need to decode that and then display."""

# Display query results images
def display_query_results(query, response, hits=5):
    query_time = response.json.get("timing", {}).get("searchtime", -1)
    query_time = round(query_time, 2)
    count = response.json.get("root", {}).get("fields", {}).get("totalCount", 0)
    html_content = f"<h3>Query text: '{query}', query time {query_time}s, count={count}, top results:</h3>"

    for i, hit in enumerate(response.hits[:hits]):
        title = hit["fields"]["title"]
        url = hit["fields"]["url"]
        page = hit["fields"]["page_number"]
        image = hit["fields"]["image"]
        score = hit["relevance"]

        html_content += f"<h4>PDF Result {i + 1}</h4>"
        html_content += f'<p><strong>Title:</strong> <a href="{url}">{title}</a>, page {page+1} with score {score:.2f}</p>'
        html_content += (
            f'<img src="data:image/png;base64,{image}" style="max-width:100%;">'
        )

    display(HTML(html_content))

"""Now we will retrieve top 5 results (this number was defined in ranking process above rerank_count=5) of the given query and see the results."""

target_hits_per_query_tensor = (
    10   # no. of initial candidates retrieved for each query tensor in the first phase.
)
async with app.asyncio(connections=1, timeout=180) as session:
    for idx, query in enumerate(queries):
        float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}
        binary_query_embeddings = dict()
        for k, v in float_query_embedding.items():
            binary_query_embeddings[k] = (
                np.packbits(np.where(np.array(v) > 0, 1, 0)).astype(np.int8).tolist()
            )

        # The mixed tensors used in MaxSim calculations
        # We use both binary and float representations
        query_tensors = {
            "input.query(qtb)": binary_query_embeddings,
            "input.query(qt)": float_query_embedding,
        }
        # The query tensors used in the nearest neighbor calculations
        for i in range(0, len(binary_query_embeddings)):
            query_tensors[f"input.query(rq{i})"] = binary_query_embeddings[i]
        nn = []
        for i in range(0, len(binary_query_embeddings)):
            nn.append(
                f"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))"
            )
        # We use a OR operator to combine the nearest neighbor operator
        nn = " OR ".join(nn)
        response: VespaQueryResponse = await session.query(
            yql=f"select title, url, image, page_number from pdf_page where {nn}",
            ranking="retrieval-and-rerank",
            timeout=120,
            hits=3,
            body={**query_tensors, "presentation.timing": True},
        )
        assert response.is_successful()
        display_query_results(query, response)

"""The top image (idx 11) has the tokens in the query "balance", "1 July 2017", "equity". So the retrieval was pretty accurate. The rest of the extracted images might not have much context considering the data is limited (70 pages only in total), and there is not much overlap of data to extract.

Now let's extract the same pdf page from pdfreader to check if the text was present in text format or was embedded in an image. There we can actually understand the realpower.
"""

# We are extracting the text of the 2nd pdf, just to showcase that pdf extractor is unable to extract much useful text from the pdf because almost everything is embedded inside an image

url = "https://www.pwc.com/jm/en/research-publications/pdf/basic-understanding-of-a-companys-financials.pdf"

pdf_file = download_pdf(pdf["url"])
pdf_file.seek(0)  # Reset file pointer for image conversion

# gets text of each page from the PDF
temp_file_path = "test.pdf"
with open(temp_file_path, "wb") as f:
    f.write(pdf_file.read())
reader = PdfReader(temp_file_path)
page_texts = []
for page_number in range(len(reader.pages)):
    page = reader.pages[page_number]
    text = page.extract_text()
    page_texts.append(text)

page_texts[11]   # as you can see when I tried extracting the same page (11th index) from pdfreader, only these text were extracted. Rest everything is embedded in image.

"""Let's visualize why the model thought the previous image has the required data, on what basis the decision was made"""

query = "balance at 1 July 2017 for equity holders"
idx = 11 # top retrieved index
top_image = pdf_lists[1]["images"][idx]
pdf_embeddings =  pdf_lists[1]["embeddings"]

# Get the number of image patches
n_patches = processor.get_n_patches(
    image_size=top_image.size,
    patch_size=model.patch_size,
    spatial_merge_size=model.spatial_merge_size,
)


# Get the tensor mask to filter out the embeddings that are not related to the image
image_mask = processor.get_image_mask(processor.process_images([top_image]))

batch_queries = processor.process_queries([query]).to(model.device)
# Generate the similarity maps
batched_similarity_maps = get_similarity_maps_from_embeddings(
    image_embeddings=pdf_embeddings[idx].unsqueeze(0).to("cuda"),
    query_embeddings=model(**batch_queries),
    n_patches=n_patches,
    image_mask=image_mask,
)

query_content = processor.decode(batch_queries.input_ids[0]).replace(processor.tokenizer.pad_token, "")
query_content = query_content.replace(processor.query_augmentation_token, "").strip()
query_tokens = processor.tokenizer.tokenize(query_content)

# Get the similarity map for our (only) input image
similarity_maps = batched_similarity_maps[0]  # (query_length, n_patches_x, n_patches_y)

print(query_tokens)  # The given query is splitted in these tokens using the processor

idx = 11  # top retrieved page from vespa
top_image = pdf_lists[1]["images"][idx]
token_idx = 2  # visualizing for 2nd index token from the query_tokens which is 'balance'

fig, ax = plot_similarity_map(
    image=top_image,
    similarity_map=similarity_maps[token_idx],
    figsize=(8, 8),
    show_colorbar=False,
)

max_sim_score = similarity_maps[token_idx, :, :].max().item()
ax.set_title(f"Token #{token_idx}: `{query_tokens[token_idx].replace('Ä ', '_')}`. MaxSim score: {max_sim_score:.2f}", fontsize=14)

del query_content, query_tokens, batch_queries, batched_similarity_maps, similarity_maps, image_mask, n_patches, top_image

idx = 11
top_image = pdf_lists[1]["images"][idx]
token_idx = 6  # visualizing for 2nd index token from the query_tokens which is 'July'

fig, ax = plot_similarity_map(
    image=top_image,
    similarity_map=similarity_maps[token_idx],
    figsize=(8, 8),
    show_colorbar=False,
)

max_sim_score = similarity_maps[token_idx, :, :].max().item()
ax.set_title(f"Token #{token_idx}: `{query_tokens[token_idx].replace('Ä ', '_')}`. MaxSim score: {max_sim_score:.2f}", fontsize=14)

del query_content, query_tokens, batch_queries, batched_similarity_maps, similarity_maps, image_mask, n_patches, top_image