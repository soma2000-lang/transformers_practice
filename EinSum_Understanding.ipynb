{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c5f426-c0e3-40fe-9775-44d2a8149245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "# from keras.engine.base_layer import Layer\n",
    "# from keras.layers import activation\n",
    "# from keras.layers import core\n",
    "# from keras.layers import regularization\n",
    "# from keras.utils import tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad0390-f70d-4819-ba24-b496f7d89a90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  einsum projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402faf8f-1761-4995-a829-56c2a566eff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "# Simulating input query tensor\n",
    "Q = tf.random.uniform(shape=[2, 8, 512])  # Shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "# Adjusting projection weights to account for num_heads and key_dim\n",
    "# New shape: [embedding_dim, num_heads, key_dim]\n",
    "W = tf.random.uniform(shape=[512, 8, 64])\n",
    "\n",
    "# Using einsum for the projection, adjusting the equation accordingly\n",
    "# The corrected equation: 'abc,cde->abde'\n",
    "# Where:\n",
    "# 'a' represents the batch size\n",
    "# 'b' represents the sequence length\n",
    "# 'c' represents the embedding dimension (to be contracted)\n",
    "# 'd' represents the number of attention heads\n",
    "# 'e' represents the key dimension\n",
    "Q_proj = tf.einsum('abc,cde->abde', Q, W)\n",
    "\n",
    "print(Q_proj.shape)  # Expected output shape: [2, 8, 8, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca586b5-b4b7-43a3-92f7-d8ef18aea746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    # Assign letters to dimensions for the einsum equation\n",
    "    # Starting letters for free dimensions\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    free_letters = letters[:free_dims]\n",
    "    \n",
    "    # Letters for bound (contracted) dimensions\n",
    "    bound_letter = letters[free_dims:free_dims + bound_dims]\n",
    "    \n",
    "    # Letters for output dimensions\n",
    "    output_letters = letters[free_dims + bound_dims:free_dims + bound_dims + output_dims]\n",
    "    \n",
    "    # Construct the input part of the equation (before \"->\")\n",
    "    input_str = free_letters + bound_letter\n",
    "    \n",
    "    # Construct the projection weights part of the equation\n",
    "    weights_str = bound_letter + output_letters\n",
    "    \n",
    "    # Construct the output part of the equation (after \"->\")\n",
    "    output_str = free_letters + output_letters\n",
    "    \n",
    "    # Combine into full einsum equation string\n",
    "    equation = f\"{input_str},{weights_str}->{output_str}\"\n",
    "    return equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efaa762-a547-41d7-b608-d6e270fa5f35",
   "metadata": {},
   "source": [
    "- free_dims = 2 (for batch_size and seq_length) \n",
    "- bound_dims = 1 (for embedding_dim, the dimension to be reduced/transformed)\n",
    "- output_dims = 2 (for num_heads and key_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29063913-ef2d-4c7c-84e4-9faa97bcd170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum equation: abc,cde->abde\n"
     ]
    }
   ],
   "source": [
    "equation = _build_proj_equation(free_dims=2, bound_dims=1, output_dims=2)\n",
    "print(\"Einsum equation:\", equation)\n",
    "# Expected output: \"abc,cde->abde\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df426c-8db3-4e71-9946-abf50d77dbd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### code from tf mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b4e5e9-d563-4306-8fd5-b4968c65ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('abc,cde->abde', 'de', 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "print(_CHR_IDX)\n",
    "\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = f\"{input_str},{kernel_str}->{output_str}\"\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "free_dims = 2\n",
    "bound_dims = 1\n",
    "output_dims = 2 \n",
    "_build_proj_equation(free_dims, bound_dims, output_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2c147-87b3-4c91-9643-6b781faed394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  scaled dot product attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdb2e639-5208-43c9-acc8-920c9a12e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention scores shape: (2, 8, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming Q and K are query and key tensors with shapes [2, 8, 10, 64]\n",
    "Q = tf.random.uniform(shape=[2, 8, 10, 64])  # Shape: [batch_size, num_heads, seq_length_q, depth]\n",
    "K = tf.random.uniform(shape=[2, 8, 10, 64])  # Shape: [batch_size, num_heads, seq_length_k, depth]\n",
    "\n",
    "# Correctly calculating the dot product between Q and K^T using einsum\n",
    "attention_scores = tf.einsum('bhqd,bhkd->bhqk', Q, K)\n",
    "\n",
    "# Scaling by 1/sqrt(depth) for normalization\n",
    "depth = 64\n",
    "scaled_attention_scores = attention_scores / tf.math.sqrt(tf.cast(depth, tf.float32))\n",
    "\n",
    "print(\"Scaled attention scores shape:\", scaled_attention_scores.shape)\n",
    "# Expected shape: [2, 8, 10, 10] matching [batch_size, num_heads, seq_length_q, seq_length_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9bc74fe-4483-4b93-abc4-7ec4eb9b8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_attention_equation(rank_q, rank_k):\n",
    "    # Correctly identifying each part's role in the einsum equation\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Assume the batch and heads dimensions are the same for Q and K and are the first two dimensions\n",
    "    base_letters = letters[:2]  # This covers batch (b) and heads (h) dimensions\n",
    "    \n",
    "    # The next two letters represent sequence length of Q (q) and sequence length of K (k)\n",
    "    seq_letter_q = letters[2]  # Third dimension for Q\n",
    "    seq_letter_k = letters[3]  # Assume next letter for K's sequence length\n",
    "    \n",
    "    # The last shared letter represents the depth dimension (d)\n",
    "    depth_letter = letters[4]  # Shared depth dimension\n",
    "    \n",
    "    # Constructing the einsum equation for the dot product attention mechanism\n",
    "    equation = f\"{base_letters}{seq_letter_q}{depth_letter},\" \\\n",
    "               f\"{base_letters}{seq_letter_k}{depth_letter}->\" \\\n",
    "               f\"{base_letters}{seq_letter_q}{seq_letter_k}\"\n",
    "    \n",
    "    return equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a8c2c8-535b-4a5f-9f94-254b200f9c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum equation for attention scores: abce,abde->abcd\n"
     ]
    }
   ],
   "source": [
    "rank_q = 4  # Rank for query tensor Q\n",
    "rank_k = 4  # Rank for key tensor K\n",
    "\n",
    "einsum_equation = _build_attention_equation(rank_q, rank_k)\n",
    "print(\"Einsum equation for attention scores:\", einsum_equation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516684fd-f137-4404-94b5-cc1501b6b59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6615d1bb-c9e2-4026-81bf-5e2d80131a65",
   "metadata": {},
   "source": [
    "###  code from tf mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0698b-0be8-4da9-a2f8-9bfe7a585b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_attention_equation(rank, attn_axes):\n",
    "    \"\"\"Builds einsum equations for the attention computation.\n",
    "\n",
    "    Query, key, value inputs after projection are expected to have the shape as:\n",
    "    `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
    "    `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
    "    For sequence data, <non-attention dims> this might be empty. However, in more complex data like images or 3D data, this could represent spatial dimensions (height, width) or other dimensions not directly involved in attention.\n",
    "\n",
    "    The attention operations can be generalized:\n",
    "    (1) Query-key dot product:\n",
    "    `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
    "    <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "    num_heads, <query attention dims>, <key attention dims>)`\n",
    "    (2) Combination:\n",
    "    `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
    "    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch\n",
    "    dims>, <query attention dims>, num_heads, channels)`\n",
    "\n",
    "    Args:\n",
    "        rank: Rank of query, key, value tensors.\n",
    "        attn_axes: List/tuple of axes, `[-1, rank)`,\n",
    "            that attention will be applied to.\n",
    "\n",
    "    Returns:\n",
    "        Einsum equations.\n",
    "    \"\"\"\n",
    "    target_notation = _CHR_IDX[:rank]\n",
    "    # `batch_dims` includes the head dim.\n",
    "    batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
    "    letter_offset = rank\n",
    "    source_notation = \"\"\n",
    "    for i in range(rank):\n",
    "        if i in batch_dims or i == rank - 1:\n",
    "            source_notation += target_notation[i]\n",
    "        else:\n",
    "            source_notation += _CHR_IDX[letter_offset]\n",
    "            letter_offset += 1\n",
    "\n",
    "    product_notation = \"\".join(\n",
    "        [target_notation[i] for i in batch_dims]\n",
    "        + [target_notation[i] for i in attn_axes]\n",
    "        + [source_notation[i] for i in attn_axes]\n",
    "    )\n",
    "    dot_product_equation = \"%s,%s->%s\" % (\n",
    "        source_notation,\n",
    "        target_notation,\n",
    "        product_notation,\n",
    "    )\n",
    "    attn_scores_rank = len(product_notation)\n",
    "    combine_equation = \"%s,%s->%s\" % (\n",
    "        product_notation,\n",
    "        source_notation,\n",
    "        target_notation,\n",
    "    )\n",
    "    return dot_product_equation, combine_equation, attn_scores_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45a839-ffee-4ce9-9af8-090e25145648",
   "metadata": {},
   "source": [
    "In linear algebra, the rank of a matrix is defined as the maximum number of linearly independent column vectors in the matrix or the maximum number of linearly independent row vectors in the matrix. Essentially, it measures the dimension of the vector space spanned by its columns or rows. The rank of a matrix can indeed be understood as the smaller of the two dimensions (columns or rows) for a full-rank matrix but more accurately, it's about linear independence:\n",
    "\n",
    "Full Rank: A matrix is considered full rank if its rank equals the smaller of its number of rows or columns.\n",
    "Rank Deficiency: A matrix is rank-deficient if it does not have full rank, meaning some of its rows or columns can be expressed as a linear combination of others.\n",
    "Rank in Tensor Operations\n",
    "In the context of tensor operations and deep learning frameworks like TensorFlow or PyTorch, the term rank is used differently. Here, the rank of a tensor simply refers to the number of dimensions (also called axes) that the tensor has. For example:\n",
    "\n",
    "A scalar (a single number) has a rank of 0.\n",
    "A vector (a 1D array of numbers) has a rank of 1.\n",
    "A matrix (a 2D array of numbers) has a rank of 2.\n",
    "A 3D array of numbers has a rank of 3, and so on.\n",
    "This usage aligns with the notion of an n-dimensional array, where \"n\" is the rank.\n",
    "\n",
    "Clarification for Tensor Shape Arguments\n",
    "When referring to the rank of query, key, value tensors in the context of building attention mechanisms or neural network layers, we're talking about how many dimensions these tensors have. Each dimension (or axis) of these tensors has a certain size (or length), which represents the extent of the tensor along that dimension. For example, a tensor shape [2, 8, 10, 64] has a rank of 4, with each number representing the size of each dimension:\n",
    "\n",
    "2 in the batch dimension,\n",
    "8 in the heads dimension,\n",
    "10 in the sequence length dimension,\n",
    "64 in the channels (or features) dimension.\n",
    "This distinction is important to keep in mind when transitioning between mathematical discussions of linear algebra and practical implementations of neural networks and tensor operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab3921-8425-4c43-98f3-9e9bd23251fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b47c0-8700-4ab7-bd3a-8cd3ad1197b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbd792-4fd4-489f-8b52-61332556b34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
