{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import sys\n\nimport os\nimport json\nimport re\nfrom  openai import OpenAI\nfrom langchain_openai import ChatOpenAI","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"with open('../secrets.json', 'r') as jsonfile:\n    configs = json.load(jsonfile)\n\nclient = OpenAI(api_key=configs.get('openai_api_key'),\n                # organization='Personal',\n                # project='proj_DqdKUU38qOGVj9Qxdq6UlXcD',\n               )","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def call_openai_api(client, messages, model=\"gpt-3.5-turbo\", max_retries=5, wait_time=5):\n    retries = 0\n    while retries < max_retries:\n        try:\n            # Make a request to the OpenAI ChatCompletion API\n           response = client.chat.completions.create(\n              model=\"gpt-4o-mini\",\n              messages=messages,\n           )\n           return response\n        except client.error.RateLimitError:\n            print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n            time.sleep(wait_time)\n            retries += 1\n    raise Exception(\"Max retries exceeded. Please check your plan and billing details.\")\n\n# Define the conversation messages\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\n\n# Call the API\nresponse = call_openai_api(client, messages)","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(response)","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"ChatCompletion(id='chatcmpl-9rFmEpNGV0XkX4enRNdD8W3HTM3gr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', role='assistant', function_call=None, tool_calls=None))], created=1722479058, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0f03d4f0ee', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31))\n"}]},{"cell_type":"code","source":"response.choices[0].message","metadata":{},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":["ChatCompletionMessage(content='The capital of France is Paris.', role='assistant', function_call=None, tool_calls=None)"]},"metadata":{}}]},{"cell_type":"code","source":"response.choices[0].message.content","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":["'The capital of France is Paris.'"]},"metadata":{}}]},{"cell_type":"markdown","source":"### langchain ChatOpenAI call ","metadata":{}},{"cell_type":"code","source":"openai_api_key = configs.get('openai_api_key')\nmessages = [\n    (\n        \"system\",\n        \"You are a helpful translator. Translate the user sentence to French.\",\n    ),\n    (\"human\", \"I love programming.\"),\n]\n\nllm = ChatOpenAI(\n    api_key=openai_api_key,\n    model=\"gpt-4o-mini-2024-07-18\",\n    # model=\"gpt-4o-mini-2024-07-18\",\n    # model=\"gpt-4o\"\n    )\nresponse = llm.invoke(messages)\nprint(response)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"content=\"J'aime programmer.\" response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 28, 'total_tokens': 32}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-a19a0001-afec-4f9f-ad12-9b1558bf2c70-0' usage_metadata={'input_tokens': 28, 'output_tokens': 4, 'total_tokens': 32}\n"}]},{"cell_type":"code","source":"response.content","metadata":{},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":["\"J'aime programmer.\""]},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    (\n        \"system\",\n        \"You are an assistant\",\n    ),\n    (\"human\", \"generate a code to read excel\"),\n]\n\nllm = ChatOpenAI(\n    api_key=openai_api_key,\n    model=\"gpt-4o-mini-2024-07-18\",\n    # model=\"gpt-4o-mini-2024-07-18\",\n    # model=\"gpt-4o\"\n    )\nresponse = llm.invoke(messages)\nprint(response)","metadata":{},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"content=\"To read an Excel file in Python, you can use the `pandas` library, which provides powerful data manipulation capabilities. If you haven't already installed `pandas`, you can do so using `pip`. You'll also need `openpyxl` or `xlrd` for reading `.xlsx` and `.xls` files, respectively.\\n\\nHere's a step-by-step guide, including an example code snippet to read an Excel file:\\n\\n### Step 1: Install Required Libraries\\n\\nIf you haven't installed `pandas` and `openpyxl`, you can do so using the following command:\\n\\n```bash\\npip install pandas openpyxl\\n```\\n\\n### Step 2: Example Code to Read Excel File\\n\\nHere's a simple example of how to read an Excel file using `pandas`:\\n\\n```python\\nimport pandas as pd\\n\\n# Specify the path to your Excel file\\nfile_path = 'path/to/your/excel_file.xlsx'\\n\\n# Read the Excel file\\n# You can specify the sheet name or sheet number (0-indexed)\\ndf = pd.read_excel(file_path, sheet_name='Sheet1')  # or sheet_name=0 for the first sheet\\n\\n# Display the first few rows of the DataFrame\\nprint(df.head())\\n```\\n\\n### Explanation:\\n\\n1. **Importing Libraries**: We import the `pandas` library.\\n2. **File Path**: Set the `file_path` variable to the location of your Excel file.\\n3. **Reading the File**: Use `pd.read_excel()` to read the Excel file. You can specify the sheet you want to read using the `sheet_name` parameter.\\n4. **Displaying Data**: The `head()` method is used to display the first few rows of the DataFrame.\\n\\n### Notes:\\n- Make sure to change `'path/to/your/excel_file.xlsx'` to the actual path where your Excel file is located.\\n- If your Excel file has multiple sheets and you want to read all of them into a dictionary of DataFrames, you can do it like this:\\n\\n```python\\nall_sheets_df = pd.read_excel(file_path, sheet_name=None)  # Reads all sheets\\n```\\n\\nThis will return a dictionary where the keys are sheet names and the values are DataFrames corresponding to each sheet. \\n\\nFeel free to ask if you need further customization or assistance!\" response_metadata={'token_usage': {'completion_tokens': 478, 'prompt_tokens': 21, 'total_tokens': 499}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-f6c6014f-7ec8-4c59-aa6f-5c8c5d93b2ef-0' usage_metadata={'input_tokens': 21, 'output_tokens': 478, 'total_tokens': 499}\n"}]},{"cell_type":"code","source":"print(response.content)","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"To read an Excel file in Python, you can use the `pandas` library, which provides powerful data manipulation capabilities. If you haven't already installed `pandas`, you can do so using `pip`. You'll also need `openpyxl` or `xlrd` for reading `.xlsx` and `.xls` files, respectively.\n\n\n\nHere's a step-by-step guide, including an example code snippet to read an Excel file:\n\n\n\n### Step 1: Install Required Libraries\n\n\n\nIf you haven't installed `pandas` and `openpyxl`, you can do so using the following command:\n\n\n\n```bash\n\npip install pandas openpyxl\n\n```\n\n\n\n### Step 2: Example Code to Read Excel File\n\n\n\nHere's a simple example of how to read an Excel file using `pandas`:\n\n\n\n```python\n\nimport pandas as pd\n\n\n\n# Specify the path to your Excel file\n\nfile_path = 'path/to/your/excel_file.xlsx'\n\n\n\n# Read the Excel file\n\n# You can specify the sheet name or sheet number (0-indexed)\n\ndf = pd.read_excel(file_path, sheet_name='Sheet1')  # or sheet_name=0 for the first sheet\n\n\n\n# Display the first few rows of the DataFrame\n\nprint(df.head())\n\n```\n\n\n\n### Explanation:\n\n\n\n1. **Importing Libraries**: We import the `pandas` library.\n\n2. **File Path**: Set the `file_path` variable to the location of your Excel file.\n\n3. **Reading the File**: Use `pd.read_excel()` to read the Excel file. You can specify the sheet you want to read using the `sheet_name` parameter.\n\n4. **Displaying Data**: The `head()` method is used to display the first few rows of the DataFrame.\n\n\n\n### Notes:\n\n- Make sure to change `'path/to/your/excel_file.xlsx'` to the actual path where your Excel file is located.\n\n- If your Excel file has multiple sheets and you want to read all of them into a dictionary of DataFrames, you can do it like this:\n\n\n\n```python\n\nall_sheets_df = pd.read_excel(file_path, sheet_name=None)  # Reads all sheets\n\n```\n\n\n\nThis will return a dictionary where the keys are sheet names and the values are DataFrames corresponding to each sheet. \n\n\n\nFeel free to ask if you need further customization or assistance!\n"}]},{"cell_type":"markdown","source":"### Handcrafted Agent ","metadata":{}},{"cell_type":"markdown","source":"#### what is agent doing\n- calling model with a prompt\n- maintianing a history of conversation","metadata":{}},{"cell_type":"code","source":"class Agent:\n    def __init__(self, system=\"\"):\n        self.system = system\n        self.messages = []\n        if self.system:\n            self.messages.append({\"role\": \"system\", \"content\": system})\n\n    def __call__(self, message):\n        self.messages.append({\"role\": \"user\", \"content\": message})\n        result = self.execute()\n        self.messages.append({\"role\": \"assistant\", \"content\": result})\n        return result\n\n    def execute(self):\n        completion = client.chat.completions.create(\n            model=\"gpt-4o-mini-2024-07-18\",\n            temperature=0,\n            messages=self.messages)\n        return completion.choices[0].message.content","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### A ReAct type prompt for agent ","metadata":{}},{"cell_type":"markdown","source":"- Thought:\n  - user asks a question to model\n  - model think about a plan of action from the available actions\n   - Available Actions:\n     - name of the python function: parameters of the function\n     - identifying the parameters correctly is critical\n- Action:\n  - from the free flow text identify the parameters for  the action\n  - return the model output as ACtion: function name: paramer\n  - This output will be parsed by the user or another program later to call the function\n  - also output 'PAUSE'. Why is this required ? may be external program can use this somehow.\n- Observation:\n  - assume that the function was called and result was obrained by user or by anothe program\n  - the model is now again called with the result along with the history\n  - the model now get to see:\n     - The original quesiton from the user\n     - The actions that it had suggested in terms of function name and its parameters\n     - the  result from the action\n- Answer: provide the final answer from the observation and the history of messages as context\n- Demonstration: give a task demonstration","metadata":{}},{"cell_type":"code","source":"agent_react_prompt = \"\"\"\nYou run in a loop of Thought, Action, PAUSE, Observation.\nAt the end of the loop you output an Answer\nUse Thought to describe your thoughts about the question you have been asked.\nUse Action to run one of the actions available to you - then return PAUSE.\nObservation will be the result of running those actions.\n\nYour available actions are:\n\nbuy_product:\ne.g. buy_product: product_name\nthe stock availale is updated with each purchase\nreturns a confirmation message to the user or a regret message when item is not in stock. \n\ncalculate_revenue:\ne.g. calculate_revenue: product_name\nreturns revenue for the product or total revenue when no product name is mentioned\n\nExample session:\n\nQuestion: I would like to buy a wireless mouse, can you despatch it please ?\nThought: I should buy the wireless mouse using the buy_product action\nAction: buy_product: wireless mouse\nPAUSE\n\nYou will be called again with this:\n\nObservation: wireless mouse purchase confirmed. Remaining stock 40.\n\nYou then output:\n\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. \nwhen the stock is not there you answer should be changed accordingly\n\"\"\".strip()","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### python functions for the agent","metadata":{}},{"cell_type":"code","source":"# E-store data\nestore_data = {\n    \"products\": [\n        {\n            \"id\": 1,\n            \"name\": \"wireless mouse\",\n            \"price\": 25.99,\n            \"quantity_available\": 100,\n            \"items_sold\": 150\n        },\n        {\n            \"id\": 2,\n            \"name\": \"mechanical keyboard\",\n            \"price\": 79.99,\n            \"quantity_available\": 50,\n            \"items_sold\": 75\n        },\n        {\n            \"id\": 3,\n            \"name\": \"usb-c hub\",\n            \"price\": 34.99,\n            \"quantity_available\": 2,\n            \"items_sold\": 120\n        }\n    ]\n}\n\ndef buy_product(product_name):\n    for product in estore_data['products']:\n        if product['name'] == product_name.lower():\n            if product['quantity_available'] > 0:\n                product['quantity_available'] -= 1\n                product['items_sold'] += 1\n                return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n            else:\n                return f\"Regret, {product_name} is not in stock.\"\n    return f\"Product {product_name} not found.\"\n\ndef calculate_revenue(product_name=None):\n    total_revenue = 0.0\n    for product in estore_data['products']:\n        if product_name and product['name'] == product_name.lower():\n            return f\"Revenue for {product_name}: ${product['items_sold'] * product['price']:.2f}\"\n        total_revenue += product['items_sold'] * product['price']\n    if product_name is None:\n        return f\"Total revenue: ${total_revenue:.2f}\"\n    return f\"Product {product_name} not found.\"\n\n# Example usage:\n# Buying a product\nprint(buy_product(\"Wireless Mouse\"))  # Wireless Mouse has been dispatched.\nprint(buy_product(\"Mechanical Keyboard\"))  # Mechanical Keyboard has been dispatched.\nprint(buy_product(\"USB-C Hub\"))  # USB-C Hub has been dispatched.\nprint(buy_product(\"Smartphone\"))  # Product Smartphone not found.\n\n# Calculating revenue\nprint(calculate_revenue(\"Wireless Mouse\"))  # Revenue for Wireless Mouse: $4158.50\nprint(calculate_revenue(\"Mechanical Keyboard\"))  # Revenue for Mechanical Keyboard: $5992.50\nprint(calculate_revenue())  # Total revenue: $17086.50\nprint(calculate_revenue(\"Smartphone\"))  # Product Smartphone not found.\nknown_actions = {\n    \"buy_product\": buy_product,\n    \"calculate_revenue\": calculate_revenue\n}","metadata":{},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Wireless Mouse purchase confirmed. Remaining stock 99\n\nMechanical Keyboard purchase confirmed. Remaining stock 49\n\nUSB-C Hub purchase confirmed. Remaining stock 1\n\nProduct Smartphone not found.\n\nRevenue for Wireless Mouse: $3924.49\n\nRevenue for Mechanical Keyboard: $6079.24\n\nTotal revenue: $14237.52\n\nProduct Smartphone not found.\n"}]},{"cell_type":"markdown","source":"### Run the agent to get the action name","metadata":{}},{"cell_type":"code","source":"agent = Agent(agent_react_prompt)","metadata":{},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# result = abot(\"How much does a toy poodle weigh?\")\nresult = agent(\"book me a order for USB-C hub\")\nprint(result)","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Thought: I need to buy a USB-C hub using the buy_product action to fulfill the order. \n\nAction: buy_product: USB-C hub\n\nPAUSE\n"}]},{"cell_type":"markdown","source":"### Parse the action name and parameters and manually recall the model for answer","metadata":{}},{"cell_type":"code","source":"def parse_action_name_n_params(result):\n    result.split('\\n')\n    pattern_action = re.compile('^Action: (\\w+): (.*)$')\n    parsed_actions = []\n    for line in result.split('\\n'):\n        action_found = pattern_action.match(line)\n        if action_found:\n            parsed_actions.append(action_found)\n    print(parsed_actions)\n    if parsed_actions:\n        action, action_input = parsed_actions[0].groups()\n        print(action, action_input )\n        return action, action_input\n    return\naction, action_input = parse_action_name_n_params(result)","metadata":{},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"[<re.Match object; span=(0, 30), match='Action: buy_product: USB-C hub'>]\n\nbuy_product USB-C hub\n"}]},{"cell_type":"code","source":"# result = average_dog_weight(\"Toy Poodle\")\nresult = known_actions[action](action_input)\nprint(result)\nnext_prompt = \"Observation: {}\".format(result)\nagent(next_prompt)","metadata":{},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"USB-C hub purchase confirmed. Remaining stock 0\n"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":["'Answer: Thanks for your order. Your USB-C hub has been despatched to you.'"]},"metadata":{}}]},{"cell_type":"markdown","source":"### Check agent is able to identify different action based on users question","metadata":{}},{"cell_type":"code","source":"# result = abot(\"what is the result of multiplication of 5 and 20?\")\nresult = agent(\"what is the revenue generated from Wireless Mouse sale? \")\nprint(result)","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Thought: I need to calculate the revenue generated from the sale of the wireless mouse using the calculate_revenue action. \n\nAction: calculate_revenue: wireless mouse\n\nPAUSE\n"}]},{"cell_type":"code","source":"action, action_input = parse_action_name_n_params(result)\nresult = known_actions[action](action_input)\nprint(result)\nnext_prompt = \"Observation: {}\".format(result)\nagent(next_prompt)","metadata":{},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"[<re.Match object; span=(0, 41), match='Action: calculate_revenue: wireless mouse'>]\n\ncalculate_revenue wireless mouse\n\nRevenue for wireless mouse: $3924.49\n"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":["'Answer: The revenue generated from the sale of the wireless mouse is $3924.49.'"]},"metadata":{}}]},{"cell_type":"markdown","source":"### Automate the Thought to Answer by calling the agent multiple times till the Final Answer","metadata":{}},{"cell_type":"code","source":"def autoagent(question, prompt=None,  max_turns=5):\n    i = 0\n    if prompt:\n        agent = Agent(prompt)\n    agent = Agent(agent_react_prompt)\n    user_prompt = question\n    while i < max_turns:\n        i += 1\n        result = agent(user_prompt)\n        print(result)\n        parse_result = parse_action_name_n_params(result)\n        if isinstance( parse_result, tuple):\n            action, action_input = parse_result\n            if action not in known_actions:\n                raise Exception(\"Unknown actions: {}: {}\".format(action, action_input))\n            print(f'calling {action}: with arguments: {action_input}')            \n            observation = known_actions[action](action_input)\n            print('observation:', observation)\n            user_prompt = \"Observation: {}\".format(observation)\n        else:\n            return    ","metadata":{},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# question = \"\"\"I have 2 dogs, a border collie and a scottish terrier. \\\n# What is their combined weight\"\"\"\nquestion = \"\"\"can you buy me one wireless mouse and one usb-c hub ? \"\"\"\nautoagent(question)","metadata":{},"execution_count":21,"outputs":[{"name":"stdout","output_type":"stream","text":"Thought: I need to buy both a wireless mouse and a USB-C hub using the buy_product action. I will first attempt to buy the wireless mouse and then the USB-C hub. \n\nAction: buy_product: wireless mouse\n\nPAUSE\n\n[<re.Match object; span=(0, 35), match='Action: buy_product: wireless mouse'>]\n\nbuy_product wireless mouse\n\ncalling buy_product: with arguments: wireless mouse\n\nobservation: wireless mouse purchase confirmed. Remaining stock 98\n\nThought: The wireless mouse purchase was successful. Now I will proceed to buy the USB-C hub. \n\nAction: buy_product: usb-c hub\n\nPAUSE\n\n[<re.Match object; span=(0, 30), match='Action: buy_product: usb-c hub'>]\n\nbuy_product usb-c hub\n\ncalling buy_product: with arguments: usb-c hub\n\nobservation: Regret, usb-c hub is not in stock.\n\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. Unfortunately, the USB-C hub is currently out of stock.\n\n[]\n"}]},{"cell_type":"markdown","source":"### Converting the python functions as langchain tool","metadata":{}},{"cell_type":"markdown","source":"#### simple tool decorator","metadata":{}},{"cell_type":"markdown","source":"name\tstr\tMust be unique within a set of tools provided to an LLM or agent.\ndescription\tstr\tDescribes what the tool does. Used as context by the LLM or agent.\nargs_schema\tPydantic BaseModel\tOptional but recommended, can be used to provide more information (e.g., few-shot examples) or validation for expected parameters","metadata":{}},{"cell_type":"code","source":"from langchain_core.tools import tool\n\n@tool\ndef buy_product(product_name):\n    \"\"\"   \n    the stock availale is updated with each purchase\n    returns a confirmation message to the user or a regret message when item is not in stock. \n    \"\"\"\n    for product in estore_data['products']:\n        if product['name'] == product_name.lower():\n            if product['quantity_available'] > 0:\n                product['quantity_available'] -= 1\n                product['items_sold'] += 1\n                return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n            else:\n                return f\"Regret, {product_name} is not in stock.\"\n    return f\"Product {product_name} not found.\"\n","metadata":{},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### type of arguments has not been detected as str, lets be more sprcific","metadata":{}},{"cell_type":"code","source":"@tool\ndef buy_product(product_name:str )->str:\n    \"\"\"   \n    the stock availale is updated with each purchase\n    returns a confirmation message to the user or a regret message when item is not in stock. \n    \"\"\"\n    for product in estore_data['products']:\n        if product['name'] == product_name.lower():\n            if product['quantity_available'] > 0:\n                product['quantity_available'] -= 1\n                product['items_sold'] += 1\n                return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n            else:\n                return f\"Regret, {product_name} is not in stock.\"\n    return f\"Product {product_name} not found.\"\nprint(\"name:\", buy_product.name)\nprint(\"description:\", buy_product.description)\nprint(\"args schema:\", buy_product.args_schema.schema())","metadata":{"scrolled":true},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"name: buy_product\n\ndescription: the stock availale is updated with each purchase\n\nreturns a confirmation message to the user or a regret message when item is not in stock.\n\nargs schema: {'title': 'buy_productSchema', 'description': 'the stock availale is updated with each purchase\\nreturns a confirmation message to the user or a regret message when item is not in stock. ', 'type': 'object', 'properties': {'product_name': {'title': 'Product Name', 'type': 'string'}}, 'required': ['product_name']}\n"}]},{"cell_type":"markdown","source":"@tool can optionally parse Google Style docstrings and associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify parse_docstring:\n\n@tool(parse_docstring=True)\ndef foo(bar: str, baz: int) -> str:\n    \"\"\"The foo.\n\n    Args:\n        bar: The bar.\n        baz: The baz.\n    \"\"\"\n    return bar","metadata":{}},{"cell_type":"markdown","source":"####  Usig Annotation-  @tool supports parsing of annotations, nested schemas","metadata":{}},{"cell_type":"code","source":"from typing import Annotated\n@tool\ndef buy_product(\n    product_name: Annotated[str, \"name of the product\"], \n    )->str:\n    \"\"\"   \n    the stock availale is updated with each purchase\n    returns a confirmation message to the user or a regret message when item is not in stock. \n    \"\"\"\n    for product in estore_data['products']:\n        if product['name'] == product_name.lower():\n            if product['quantity_available'] > 0:\n                product['quantity_available'] -= 1\n                product['items_sold'] += 1\n                return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n            else:\n                return f\"Regret, {product_name} is not in stock.\"\n    return f\"Product {product_name} not found.\"\nprint(\"name:\", buy_product.name)\nprint(\"description:\", buy_product.description)\nprint(\"args schema:\", buy_product.args_schema.schema())","metadata":{},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"name: buy_product\n\ndescription: the stock availale is updated with each purchase\n\nreturns a confirmation message to the user or a regret message when item is not in stock.\n\nargs schema: {'title': 'buy_productSchema', 'description': 'the stock availale is updated with each purchase\\nreturns a confirmation message to the user or a regret message when item is not in stock. ', 'type': 'object', 'properties': {'product_name': {'title': 'Product Name', 'description': 'name of the product', 'type': 'string'}}, 'required': ['product_name']}\n"}]},{"cell_type":"markdown","source":"#### Using Pydantic ","metadata":{}},{"cell_type":"code","source":"from langchain.pydantic_v1 import BaseModel, Field\n\nclass BuypoductInputSchema(BaseModel):\n    product_name: str = Field(description=\"name of the product to be purchased\")\n\n@tool(\"buy_product-tool\", args_schema=BuypoductInputSchema, return_direct=True) \n### return_direct returns the output directly instead of sending it back to model\ndef buy_product(product_name:str )->str:\n    \"\"\"   \n    the stock availale is updated with each purchase\n    returns a confirmation message to the user or a regret message when item is not in stock. \n    \"\"\"\n    for product in estore_data['products']:\n        if product['name'] == product_name.lower():\n            if product['quantity_available'] > 0:\n                product['quantity_available'] -= 1\n                product['items_sold'] += 1\n                return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n            else:\n                return f\"Regret, {product_name} is not in stock.\"\n    return f\"Product {product_name} not found.\"\nprint(\"name:\", buy_product.name)\nprint(\"description:\", buy_product.description)\nprint(\"args schema:\", buy_product.args_schema.schema())","metadata":{},"execution_count":25,"outputs":[{"name":"stdout","output_type":"stream","text":"name: buy_product-tool\n\ndescription: the stock availale is updated with each purchase\n\nreturns a confirmation message to the user or a regret message when item is not in stock.\n\nargs schema: {'title': 'BuypoductInputSchema', 'type': 'object', 'properties': {'product_name': {'title': 'Product Name', 'description': 'name of the product to be purchased', 'type': 'string'}}, 'required': ['product_name']}\n"}]},{"cell_type":"markdown","source":"#### StructuredTool","metadata":{}},{"cell_type":"code","source":"from langchain_core.tools import StructuredTool\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n\nasync def amultiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n\ncalculator = StructuredTool.from_function(func=multiply, \n                                          # name=\"Calculator\",\n                                          # description=\"multiply numbers\",\n                                          # args_schema=CalculatorInput, ## pydantic class\n                                          coroutine=amultiply)\n\nprint(calculator.invoke({\"a\": 2, \"b\": 3}))\nprint(await calculator.ainvoke({\"a\": 2, \"b\": 5}))","metadata":{},"execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"6\n\n10\n"}]},{"cell_type":"markdown","source":"#### Subclass BaseTool","metadata":{}},{"cell_type":"code","source":"# https://api.python.langchain.com/en/latest/tools/langchain_core.tools.BaseTool.html\nfrom typing import Optional, Type\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.tools import BaseTool\nbuy_product_description = \"\"\" the stock availale is updated with each purchase.\n    returns a confirmation message to the user or a regret message when item is not in stock. \n    \"\"\"\n\nclass BuyProductTool(BaseTool):\n    name = \"buy_product\"\n    description = buy_product_description\n    args_schema: Type[BaseModel] = BuypoductInputSchema  ## created previously using pydantic\n    return_direct: bool = True\n\n    def _run(self, product_name: str, \n             run_manager: Optional[CallbackManagerForToolRun]=None,\n            ) -> str:\n        for product in estore_data['products']:\n            if product['name'] == product_name.lower():\n                if product['quantity_available'] > 0:\n                    product['quantity_available'] -= 1\n                    product['items_sold'] += 1\n                    return f\"{product_name} purchase confirmed. Remaining stock {product['quantity_available']}\"\n                else:\n                    return f\"Regret, {product_name} is not in stock.\"\n        return f\"Product {product_name} not found.\"\n\n    async def _arun(self, product_name: str, \n             run_manager: Optional[AsyncCallbackManagerForToolRun]=None,\n            ) -> str:\n         # If the calculation is cheap, you can just delegate to the sync implementation\n         # as shown below.\n         # If the sync calculation is expensive, you should delete the entire _arun method.\n         # LangChain will automatically provide a better implementation that will\n         # kick off the task in a thread to make sure it doesn't block other async code.\n         return self._run(product_name, run_manager=run_manager.get_sync())\n\nbuyproduct_tool = BuyProductTool()\nprint(\"name:\", buyproduct_tool.name)\nprint(\"description:\", buyproduct_tool.description)\nprint(\"args schema:\", buyproduct_tool.args)   \nprint('invoking sync ...:', buyproduct_tool.invoke('mechanical keyboard'))\nprint('invoking async ...:', await  buyproduct_tool.ainvoke('mechanical keyboard'))","metadata":{},"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":"name: buy_product\n\ndescription:  the stock availale is updated with each purchase.\n\n    returns a confirmation message to the user or a regret message when item is not in stock. \n\n    \n\nargs schema: {'product_name': {'title': 'Product Name', 'description': 'name of the product to be purchased', 'type': 'string'}}\n\ninvoking sync ...: mechanical keyboard purchase confirmed. Remaining stock 48\n\ninvoking async ...: mechanical keyboard purchase confirmed. Remaining stock 47\n"}]},{"cell_type":"markdown","source":"#### Creating tools from Runnables","metadata":{}},{"cell_type":"code","source":"from langchain_core.language_models import GenericFakeChatModel\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages(\n    [(\"human\", \"Hello. Please respond in the style of {answer_style}.\")]\n)\n\n# Placeholder LLM\nllm = GenericFakeChatModel(messages=iter([\"hello matey\"]))\n\nchain = prompt | llm | StrOutputParser()\n\nas_tool = chain.as_tool(\n    name=\"Style responder\", description=\"Description of when to use tool.\"\n)\nas_tool.args","metadata":{},"execution_count":28,"outputs":[{"name":"stderr","output_type":"stream","text":"/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n\n  warn_beta(\n"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":["{'answer_style': {'title': 'Answer Style', 'type': 'string'}}"]},"metadata":{}}]},{"cell_type":"markdown","source":"#### tool for the calculate revenue with handle_tool_error exception","metadata":{}},{"cell_type":"markdown","source":"Sometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\nThe Tool and ToolMessage interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).\nIf we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool.\nREQUIRES langchain-core >= 0.2.19","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"from langchain_core.tools import ToolException\nfrom typing import Union, Callable, Literal\n\nclass CalcRevInputSchema(BaseModel):\n    product_name: str = Field(description=\"name of the product to be purchased\")\n\ncalculate_revenue_description = \"\"\" returns revenue for the product or total revenue when no product name is mentione\n    \"\"\"\n\nclass CalculateRevenueTool(BaseTool):\n    name = \"calculate_revenue\"\n    description = calculate_revenue_description\n    args_schema: Type[BaseModel] = CalcRevInputSchema\n    return_direct: bool = True ## the AgentExecutor will stop looping, output does not go to model\n    # handle_tool_error = True ## or a string message or a callable function. will not work. use typing \n    handle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = True\n    response_format: Literal['content', 'content_and_artifact'] = 'content_and_artifact' ## or content\n    ## metadata: Optional[Dict[str, Any]] = None , ###Optional metadata associated with the tool. Defaults to None. This metadata will be associated with each call to this tool, and passed as arguments to the handlers defined in callbacks. You can use these to eg identify a specific instance of a tool with its use case.\n    verbose: bool = True\n\n    \n    def _run(self, product_name: str=None, \n             run_manager: Optional[CallbackManagerForToolRun]=None,\n            ) -> str:\n        total_revenue = 0.0\n        artifact_prodct_wise_rev = {}\n        for product in estore_data['products']:\n            artifact_prodct_wise_rev[product[\"name\"]] = product['items_sold'] * product['price']\n        for product in estore_data['products']:\n            if product_name and product['name'] == product_name.lower():\n                content = f\"Revenue for {product_name}: ${product['items_sold'] * product['price']:.2f}\"\n                return content, artifact_prodct_wise_rev\n            total_revenue += product['items_sold'] * product['price']\n        if product_name is None:\n            content = f\"Total revenue: ${total_revenue:.2f}\"\n            return content, artifact_prodct_wise_rev\n        # return f\"Product {product_name} not found.\" instead we will use ToolException        \n        raise ToolException(f\"Product {product_name} not found.\")\n\n    async def _arun(self, product_name: str=None, \n             run_manager: Optional[AsyncCallbackManagerForToolRun]=None,\n            ) -> str:         \n         return self._run(product_name, run_manager=run_manager.get_sync())\n\ncalculate_revenue_tool = CalculateRevenueTool()\nprint(\"name:\", calculate_revenue_tool.name)\nprint(\"description:\", calculate_revenue_tool.description)\nprint(\"args schema:\", calculate_revenue_tool.args)\nprint('invoking sync with ToolCall format to get ToolMessage ...:' )\ntool_message = calculate_revenue_tool.invoke(\n    {\n        \"name\": \"calculate_revenue_tool\",\n        \"args\": {\"product_name\": \"mechanical keyboard\"},\n        \"id\": \"1\" , ## required field\n        \"type\": \"tool_call\", ## required field\n    }\n)     \nprint('tool_message ...:', tool_message)\nprint('invoking async ...:', await  calculate_revenue_tool.ainvoke('mechanical keyboard'))\nprint('handle_tool_error ToolException sync ...:', calculate_revenue_tool.invoke('mechanical keyboard'))","metadata":{},"execution_count":29,"outputs":[{"name":"stdout","output_type":"stream","text":"name: calculate_revenue\n\ndescription:  returns revenue for the product or total revenue when no product name is mentione\n\n    \n\nargs schema: {'product_name': {'title': 'Product Name', 'description': 'name of the product to be purchased', 'type': 'string'}}\n\ninvoking sync with ToolCall format to get ToolMessage ...:\n\n\u001b[32;1m\u001b[1;3mcontent='Revenue for mechanical keyboard: $6239.22' name='calculate_revenue' tool_call_id='1' artifact={'wireless mouse': 3950.4799999999996, 'mechanical keyboard': 6239.219999999999, 'usb-c hub': 4268.780000000001}\u001b[0mtool_message ...: content='Revenue for mechanical keyboard: $6239.22' name='calculate_revenue' tool_call_id='1' artifact={'wireless mouse': 3950.4799999999996, 'mechanical keyboard': 6239.219999999999, 'usb-c hub': 4268.780000000001}\n\n\u001b[32;1m\u001b[1;3mRevenue for mechanical keyboard: $6239.22\u001b[0minvoking async ...: Revenue for mechanical keyboard: $6239.22\n\n\u001b[32;1m\u001b[1;3mRevenue for mechanical keyboard: $6239.22\u001b[0mhandle_tool_error ToolException sync ...: Revenue for mechanical keyboard: $6239.22\n"}]},{"cell_type":"markdown","source":"### Use the tool inside prompt, instead of hard coded prompt","metadata":{}},{"cell_type":"code","source":"calculate_revenue_tool.args","metadata":{},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":["{'product_name': {'title': 'Product Name',\n","  'description': 'name of the product to be purchased',\n","  'type': 'string'}}"]},"metadata":{}}]},{"cell_type":"code","source":"args_list = list(calculate_revenue_tool.args.keys())\nargs_list","metadata":{},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":["['product_name']"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List\n# tools = [CalculateRevenueTool(), BuyProductTool()]\ntools = [buyproduct_tool, calculate_revenue_tool]\n\ndef generate_action_prompt(tools: List):\n    action_list_txt = \"\"\n    for tool in tools:\n        args_list = list(tool.args.keys())\n        args_as_text = \", \".join(args_list)\n        action_signature = f\"{tool.name}: {args_as_text} \\n\"\n        action_description = f\"{tool.description}\\n\\n\"\n        action_list_txt += action_signature + action_description\n    return action_list_txt\naction_list_txt = generate_action_prompt(tools)\nprint(action_list_txt)        ","metadata":{},"execution_count":32,"outputs":[{"name":"stdout","output_type":"stream","text":"buy_product: product_name \n\n the stock availale is updated with each purchase.\n\n    returns a confirmation message to the user or a regret message when item is not in stock. \n\n    \n\n\n\ncalculate_revenue: product_name \n\n returns revenue for the product or total revenue when no product name is mentione\n\n    \n\n\n\n\n"}]},{"cell_type":"code","source":"react_instruction_str = \"\"\"You run in a loop of Thought, Action, PAUSE, Observation.\nAt the end of the loop you output an Answer\nUse Thought to describe your thoughts about the question you have been asked.\nUse Action to run one of the actions available to you - then return PAUSE.\nObservation will be the result of running those actions.\n\nYour available actions are:\"\"\"\n\nexample_session_str = \"\"\"Example session:\n\nQuestion: I would like to buy a wireless mouse, can you despatch it please ?\nThought: I should buy the wireless mouse using the buy_product action\nAction: buy_product: wireless mouse\nPAUSE\n\nYou will be called again with this:\n\nObservation: wireless mouse purchase confirmed. Remaining stock 40.\n\nYou then output:\n\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. \nwhen the stock is not there you answer should be changed accordingly\"\"\"","metadata":{},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"sys_template_string_agent = \"\"\"\n{react_instruction_str}\n\n{action_list_txt}\n\n{example_session_str}\n\"\"\"\nagent_react_prompt_with_var = sys_template_string_agent.format(\n    react_instruction_str = react_instruction_str,\n    action_list_txt=action_list_txt,\n    example_session_str=example_session_str).strip()\nprint(agent_react_prompt_with_var)","metadata":{},"execution_count":34,"outputs":[{"name":"stdout","output_type":"stream","text":"You run in a loop of Thought, Action, PAUSE, Observation.\n\nAt the end of the loop you output an Answer\n\nUse Thought to describe your thoughts about the question you have been asked.\n\nUse Action to run one of the actions available to you - then return PAUSE.\n\nObservation will be the result of running those actions.\n\n\n\nYour available actions are:\n\n\n\nbuy_product: product_name \n\n the stock availale is updated with each purchase.\n\n    returns a confirmation message to the user or a regret message when item is not in stock. \n\n    \n\n\n\ncalculate_revenue: product_name \n\n returns revenue for the product or total revenue when no product name is mentione\n\n    \n\n\n\n\n\n\n\nExample session:\n\n\n\nQuestion: I would like to buy a wireless mouse, can you despatch it please ?\n\nThought: I should buy the wireless mouse using the buy_product action\n\nAction: buy_product: wireless mouse\n\nPAUSE\n\n\n\nYou will be called again with this:\n\n\n\nObservation: wireless mouse purchase confirmed. Remaining stock 40.\n\n\n\nYou then output:\n\n\n\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. \n\nwhen the stock is not there you answer should be changed accordingly\n"}]},{"cell_type":"code","source":"question = \"\"\"can you buy me one wireless mouse and one usb-c hub ? \"\"\"\nautoagent(question, prompt=agent_react_prompt_with_var)","metadata":{},"execution_count":35,"outputs":[{"name":"stdout","output_type":"stream","text":"Thought: I need to buy both a wireless mouse and a USB-C hub using the buy_product action. I will first attempt to buy the wireless mouse and then the USB-C hub. \n\nAction: buy_product: wireless mouse\n\nPAUSE\n\n[<re.Match object; span=(0, 35), match='Action: buy_product: wireless mouse'>]\n\nbuy_product wireless mouse\n\ncalling buy_product: with arguments: wireless mouse\n\nobservation: wireless mouse purchase confirmed. Remaining stock 97\n\nThought: The wireless mouse purchase was successful. Now I will proceed to buy the USB-C hub. \n\nAction: buy_product: usb-c hub\n\nPAUSE\n\n[<re.Match object; span=(0, 30), match='Action: buy_product: usb-c hub'>]\n\nbuy_product usb-c hub\n\ncalling buy_product: with arguments: usb-c hub\n\nobservation: Regret, usb-c hub is not in stock.\n\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. Unfortunately, the USB-C hub is currently out of stock.\n\n[]\n"}]},{"cell_type":"code","source":"llm.bind_tools","metadata":{},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":["<bound method BaseChatModel.bind_tools of GenericFakeChatModel(messages=<list_iterator object at 0x7f63b4421790>)>"]},"metadata":{}}]},{"cell_type":"markdown","source":"For the models that use tool calling, no special prompting is needed. ???","metadata":{}},{"cell_type":"markdown","source":"###  Understanding Langchain Prompt","metadata":{}},{"cell_type":"code","source":"print(ChatPromptTemplate.__doc__)","metadata":{},"execution_count":37,"outputs":[{"name":"stdout","output_type":"stream","text":"Prompt template for chat models.\n\n\n\n    Use to create flexible templated prompts for chat models.\n\n\n\n    Examples:\n\n\n\n        .. versionchanged:: 0.2.24\n\n\n\n            You can pass any Message-like formats supported by\n\n            ``ChatPromptTemplate.from_messages()`` directly to ``ChatPromptTemplate()``\n\n            init.\n\n\n\n        .. code-block:: python\n\n\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n\n\n            template = ChatPromptTemplate([\n\n                (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n\n                (\"human\", \"Hello, how are you doing?\"),\n\n                (\"ai\", \"I'm doing well, thanks!\"),\n\n                (\"human\", \"{user_input}\"),\n\n            ])\n\n\n\n            prompt_value = template.invoke(\n\n                {\n\n                    \"name\": \"Bob\",\n\n                    \"user_input\": \"What is your name?\"\n\n                }\n\n            )\n\n            # Output:\n\n            # ChatPromptValue(\n\n            #    messages=[\n\n            #        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n\n            #        HumanMessage(content='Hello, how are you doing?'),\n\n            #        AIMessage(content=\"I'm doing well, thanks!\"),\n\n            #        HumanMessage(content='What is your name?')\n\n            #    ]\n\n            #)\n\n\n\n    Messages Placeholder:\n\n\n\n        .. code-block:: python\n\n\n\n            # In addition to Human/AI/Tool/Function messages,\n\n            # you can initialize the template with a MessagesPlaceholder\n\n            # either using the class directly or with the shorthand tuple syntax:\n\n\n\n            template = ChatPromptTemplate([\n\n                (\"system\", \"You are a helpful AI bot.\"),\n\n                # Means the template will receive an optional list of messages under\n\n                # the \"conversation\" key\n\n                (\"placeholder\", \"{conversation}\")\n\n                # Equivalently:\n\n                # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n\n            ])\n\n\n\n            prompt_value = template.invoke(\n\n                {\n\n                    \"conversation\": [\n\n                        (\"human\", \"Hi!\"),\n\n                        (\"ai\", \"How can I assist you today?\"),\n\n                        (\"human\", \"Can you make me an ice cream sundae?\"),\n\n                        (\"ai\", \"No.\")\n\n                    ]\n\n                }\n\n            )\n\n\n\n            # Output:\n\n            # ChatPromptValue(\n\n            #    messages=[\n\n            #        SystemMessage(content='You are a helpful AI bot.'),\n\n            #        HumanMessage(content='Hi!'),\n\n            #        AIMessage(content='How can I assist you today?'),\n\n            #        HumanMessage(content='Can you make me an ice cream sundae?'),\n\n            #        AIMessage(content='No.'),\n\n            #    ]\n\n            #)\n\n\n\n    Single-variable template:\n\n\n\n        If your prompt has only a single input variable (i.e., 1 instance of \"{variable_nams}\"),\n\n        and you invoke the template with a non-dict object, the prompt template will\n\n        inject the provided argument into that variable location.\n\n\n\n\n\n        .. code-block:: python\n\n\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n\n\n            template = ChatPromptTemplate([\n\n                (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n\n                (\"human\", \"{user_input}\"),\n\n            ])\n\n\n\n            prompt_value = template.invoke(\"Hello, there!\")\n\n            # Equivalent to\n\n            # prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\n\n\n\n            # Output:\n\n            #  ChatPromptValue(\n\n            #     messages=[\n\n            #         SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n\n            #         HumanMessage(content='Hello, there!'),\n\n            #     ]\n\n            # )\n\n\n\n    \n"}]},{"cell_type":"code","source":"!pip list|grep langchain","metadata":{},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"langchain                 0.2.11\n\nlangchain-community       0.2.10\n\nlangchain-core            0.2.25\n\nlangchain-experimental    0.0.63\n\nlangchain-openai          0.1.17\n\nlangchain-text-splitters  0.2.2\n\n\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n\nYou should consider upgrading via the '/mnt/d/myDev/llmapps/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"}]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate","metadata":{},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot.\"),\n                # Means the template will receive an optional list of messages under\n                # the \"conversation\" key\n                (\"placeholder\", \"{conversation}\")\n                # Equivalently:\n                # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n            ])\nprint(template)\nprompt_value = template.invoke(\n    {\n        \"conversation\": [\n            (\"human\", \"Hi!\"),\n            (\"ai\", \"How can I assist you today?\"),\n            (\"human\", \"Can you make me an ice cream sundae?\"),\n            (\"ai\", \"No.\")\n        ]\n    }\n)\nprompt_value","metadata":{},"execution_count":40,"outputs":[{"name":"stdout","output_type":"stream","text":"input_variables=[] optional_variables=['conversation'] input_types={'conversation': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'conversation': []} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful AI bot.')), MessagesPlaceholder(variable_name='conversation', optional=True)]\n"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot.'), HumanMessage(content='Hi!'), AIMessage(content='How can I assist you today?'), HumanMessage(content='Can you make me an ice cream sundae?'), AIMessage(content='No.')])"]},"metadata":{}}]},{"cell_type":"code","source":"template = ChatPromptTemplate([\n    (\"system\", \"You are a helpful AI bot. Your name is {bot_name}.\"),\n    (\"human\", \"Hello, how are you doing?\"),\n    (\"ai\", \"{greetings}, I'm doing well, thanks!\"),\n    (\"human\", \"{user_input}\"),\n    ],\n    input_variables=['user_input'],\n    optional_variables=[\"greetings\"],\n    partial_variables={\"bot_name\": \"Monalisa\"}                            \n\n)\nprint(template)\nfinal_input = {            \n            \"user_input\": \"What is your name?\"\n        }\ntry:\n    prompt_value = template.invoke(final_input)\nexcept Exception as e:\n    print(e)\n# # print(prompt_value)\n# template_partial = template.partial(bot_name=\"Monalisa\")\n# print(template_partial)\n# try:\n#     template_partial.invoke(final_input)\n# except Exception as e:\n#     print(e)","metadata":{},"execution_count":41,"outputs":[{"name":"stdout","output_type":"stream","text":"input_variables=['greetings', 'user_input'] optional_variables=['greetings'] partial_variables={'bot_name': 'Monalisa'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['bot_name'], template='You are a helpful AI bot. Your name is {bot_name}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Hello, how are you doing?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['greetings'], template=\"{greetings}, I'm doing well, thanks!\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))]\n\n\"Input to ChatPromptTemplate is missing variables {'greetings'}.  Expected: ['greetings', 'user_input'] Received: ['user_input']\"\n"}]},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant\"),\n    (\"user\", \"Tell me a joke about {topic}\")\n])\n\nprompt_template.invoke({\"topic\": \"cats\"})","metadata":{},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])"]},"metadata":{}}]},{"cell_type":"code","source":"from langchain_core.prompts import MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n\nprompt_template = ChatPromptTemplate.from_messages([\n    # (\"system\", \"You are a helpful assistant\"),\n    SystemMessage(content='You are a helpful assistant'),\n    MessagesPlaceholder(\"msgs\")\n])\n\nprompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you\")]})","metadata":{},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!'), HumanMessage(content='How are you')])"]},"metadata":{}}]},{"cell_type":"code","source":"from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate","metadata":{},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"lc_sys_prompt_agent = SystemMessagePromptTemplate.from_template(sys_template_string_agent)\nlc_sys_prompt_agent","metadata":{},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":["SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_list_txt', 'example_session_str', 'react_instruction_str'], template='\\n{react_instruction_str}\\n\\n{action_list_txt}\\n\\n{example_session_str}\\n'))"]},"metadata":{}}]},{"cell_type":"code","source":"print(lc_sys_prompt_agent.prompt.input_variables)\nprint(lc_sys_prompt_agent.prompt.template)\nprint(lc_sys_prompt_agent.format(\n    react_instruction_str = react_instruction_str,\n    action_list_txt=action_list_txt,\n    example_session_str=example_session_str))","metadata":{},"execution_count":46,"outputs":[{"name":"stdout","output_type":"stream","text":"['action_list_txt', 'example_session_str', 'react_instruction_str']\n\n\n\n{react_instruction_str}\n\n\n\n{action_list_txt}\n\n\n\n{example_session_str}\n\n\n\ncontent='\\nYou run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\nbuy_product: product_name \\n the stock availale is updated with each purchase.\\n    returns a confirmation message to the user or a regret message when item is not in stock. \\n    \\n\\ncalculate_revenue: product_name \\n returns revenue for the product or total revenue when no product name is mentione\\n    \\n\\n\\n\\nExample session:\\n\\nQuestion: I would like to buy a wireless mouse, can you despatch it please ?\\nThought: I should buy the wireless mouse using the buy_product action\\nAction: buy_product: wireless mouse\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: wireless mouse purchase confirmed. Remaining stock 40.\\n\\nYou then output:\\n\\nAnswer: Thanks for your order. Your wireless mouse has been despatched to you. \\nwhen the stock is not there you answer should be changed accordingly\\n'\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Agent:\n    def __init__(self, system=\"\"):\n        self.system = system\n        self.messages = []\n        if self.system:\n            self.messages.append({\"role\": \"system\", \"content\": system})\n\n    def __call__(self, message):\n        self.messages.append({\"role\": \"user\", \"content\": message})\n        result = self.execute()\n        self.messages.append({\"role\": \"assistant\", \"content\": result})\n        return result\n\n    def execute(self):\n        completion = client.chat.completions.create(\n            model=\"gpt-4o-mini-2024-07-18\",\n            temperature=0,\n            messages=self.messages)\n        return completion.choices[0].message.content","metadata":{},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"https://lilianweng.github.io/posts/2023-06-23-agent/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}