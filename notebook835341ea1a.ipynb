{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# importing required libraries\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport math,copy,re\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport torchtext\nimport matplotlib.pyplot as plt\nwarnings.simplefilter(\"ignore\")\nprint(torch.__version__)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T16:18:44.712679Z","iopub.execute_input":"2024-08-20T16:18:44.713059Z","iopub.status.idle":"2024-08-20T16:18:52.365536Z","shell.execute_reply.started":"2024-08-20T16:18:44.713026Z","shell.execute_reply":"2024-08-20T16:18:52.364320Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.1.2+cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"First of all we need to convert each word in the input sequence to an embedding vector. Embedding vectors will create a more semantic representation of each word.\n\nSuppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. These marix will be learned on training and during inference each word will be mapped to corresponding 512 d vector. Suppose we have batch size of 32 and sequence length of 10(10 words). The the output will be 32x10x512.","metadata":{}},{"cell_type":"code","source":"class Embedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        \"\"\"\n        Args:\n            vocab_size: size of vocabulary\n            embed_dim: dimension of embeddings\n        \"\"\"\n        super(Embedding, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input vector\n        Returns:\n            out: embedding vector\n        \"\"\"\n        out = self.embed(x)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T16:20:15.106509Z","iopub.execute_input":"2024-08-20T16:20:15.107768Z","iopub.status.idle":"2024-08-20T16:20:15.115005Z","shell.execute_reply.started":"2024-08-20T16:20:15.107723Z","shell.execute_reply":"2024-08-20T16:20:15.113543Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"pos -> refers to order in the sentence\ni -> refers to position along embedding vector dimension","metadata":{}},{"cell_type":"markdown","source":"Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 and it is added with the correspondng positional vector which is of dimension 1 x 512 to get 1 x 512 dim out for each word/token.\n\nfor eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 32 x 10 x 512. Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both.","metadata":{}},{"cell_type":"code","source":"\n\nclass Embedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        \"\"\"\n        Args:\n            vocab_size: size of vocabulary\n            embed_dim: dimension of embeddings\n        \"\"\"\n        super(Embedding, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input vector\n        Returns:\n            out: embedding vector\n        \"\"\"\n        out = self.embed(x)\n        return out\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass PositionalEmbedding(nn.Module):\n    def __init__(self,max_seq_len,embed_model_dim):\n        \"\"\"\n        Args:\n            seq_len: length of input sequence\n            embed_model_dim: demension of embedding\n        \"\"\"\n        super(PositionalEmbedding, self).__init__()\n        self.embed_dim = embed_model_dim\n\n        pe = torch.zeros(max_seq_len,self.embed_dim)\n        for pos in range(max_seq_len):\n            for i in range(0,self.embed_dim,2):\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: input vector\n        Returns:\n            x: output\n        \"\"\"\n      \n        # make embeddings relatively larger\n        x = x * math.sqrt(self.embed_dim)\n        #add constant to embedding\n        seq_len = x.size(1)\n        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n        return x\n               ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim=512, n_heads=8):\n        \"\"\"\n        Args:\n            embed_dim: dimension of embeding vector output\n            n_heads: number of self attention heads\n        \"\"\"\n        super(MultiHeadAttention, self).__init__()\n\n        self.embed_dim = embed_dim    #512 dim\n        self.n_heads = n_heads   #8\n        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n       \n        #key,query and value matrixes    #64 x 64   \n        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n        self.embed_dim = embed_dim    #512 dim\n        self.n_heads = n_heads   #8\n        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n       \n        #key,query and value matrixes    #64 x 64   \n        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n          def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n        \n        \"\"\"\n        Args:\n           key : key vector\n           query : query vector\n           value : value vector\n           mask: mask for decoder\n        \n        Returns:\n           output vector from multihead attention\n        \"\"\"\n        batch_size = key.size(0)\n        seq_length = key.size(1)\n        \n        # query dimension can change in decoder during inference. \n        # so we cant take general seq_length\n        seq_length_query = query.size(1)\n        \n        # 32x10x512\n        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n       \n        k = self.key_matrix(key)       # (32x10x8x64)\n        q = self.query_matrix(query)   \n        v = self.value_matrix(value)\n\n        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n       \n        # computes attention\n        # adjust key for matrix multiplication\n        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n      \n        \n        # fill those positions of product matrix as (-1e20) where mask positions are 0\n        if mask is not None:\n             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n\n        #divising by square root of key dimension\n        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n\n        #applying softmax\n        scores = F.softmax(product, dim=-1)\n \n        #mutiply with value matrix\n        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n        \n        #concatenated output\n        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n        \n        output = self.out(concat) #(32,10,512) -> (32,10,512)\n       \n        return output\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n        super(TransformerBlock, self).__init__()\n        \n        \"\"\"\n        Args:\n           embed_dim: dimension of the embedding\n           expansion_factor: fator ehich determines output dimension of linear layer\n           n_heads: number of attention heads\n        \n        \"\"\"\n        self.attention = MultiHeadAttention(embed_dim, n_heads)\n        \n        self.norm1 = nn.LayerNorm(embed_dim) \n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        self.feed_forward = nn.Sequential(\n                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n                          nn.ReLU(),\n                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n        )\n\n        self.dropout1 = nn.Dropout(0.2)\n        self.dropout2 = nn.Dropout(0.2)\n\n    def forward(self,key,query,value):\n        \n        \"\"\"\n        Args:\n           key: key vector\n           query: query vector\n           value: value vector\n           norm2_out: output of transformer block\n        \n        \"\"\"\n        \n        attention_out = self.attention(key,query,value)  #32x10x512\n        attention_residual_out = attention_out + value  #32x10x512\n        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #32x10x512\n\n        feed_fwd_out = self.feed_forward(norm1_out) #32x10x512 -> #32x10x2048 -> 32x10x512\n        feed_fwd_residual_out = feed_fwd_out + norm1_out #32x10x512\n        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #32x10x512\n\n        return norm2_out\n\n\n\nclass TransformerEncoder(nn.Module):\n    \"\"\"\n    Args:\n        seq_len : length of input sequence\n        embed_dim: dimension of embedding\n        num_layers: number of encoder layers\n        expansion_factor: factor which determines number of linear layers in feed forward layer\n        n_heads: number of heads in multihead attention\n        \n    Returns:\n        out: output of the encoder\n    \"\"\"\n    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n        super(TransformerEncoder, self).__init__()\n        \n        self.embedding_layer = Embedding(vocab_size, embed_dim)\n        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n\n        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n    \n    def forward(self, x):\n        embed_out = self.embedding_layer(x)\n        out = self.positional_encoder(embed_out)\n        for layer in self.layers:\n            out = layer(out,out,out)\n\n        return out  #32x10x512\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n5. Decoder\n\nNow we have gone through most parts of the encoder.Let us get in to the components of the decoder. We will use the output of encoder to generate key and value vectors for the decoder.There are two kinds of multi head attention in the decoder.One is the decoder attention and other is the encoder decoder attention. Don't worry we will go step by step.\n\nLet us explain with respect to the training phase. Firt\n\nStep 1:\n\nFirst the output gets passed through the embeddin and positional encoding to create a embedding vector of dimension 1x512 corresponding to each word in the target sequence.\n\n","metadata":{}},{"cell_type":"code","source":"\nclass TransformerDecoder(nn.Module):\n    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n        super(TransformerDecoder, self).__init__()\n        \"\"\"  \n        Args:\n           target_vocab_size: vocabulary size of taget\n           embed_dim: dimension of embedding\n           seq_len : length of input sequence\n           num_layers: number of encoder layers\n           expansion_factor: factor which determines number of linear layers in feed forward layer\n           n_heads: number of heads in multihead attention\n        \n        \"\"\"\n        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n\n        self.layers = nn.ModuleList(\n            [\n                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n                for _ in range(num_layers)\n            ]\n\n        )\n        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x, enc_out, mask):\n        \n        \"\"\"\n        Args:\n            x: input vector from target\n            enc_out : output from encoder layer\n            trg_mask: mask for decoder self attention\n        Returns:\n            out: output vector\n        \"\"\"\n            \n        \n        x = self.word_embedding(x)  #32x10x512\n        x = self.position_embedding(x) #32x10x512\n        x = self.dropout(x)\n     \n        for layer in self.layers:\n            x = layer(enc_out, x, enc_out, mask) \n\n        out = F.softmax(self.fc_out(x))\n\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nFinally we will arrange all submodules and creates the entire tranformer architecture.\n","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n        super(Transformer, self).__init__()\n        \n        \"\"\"  \n        Args:\n           embed_dim:  dimension of embedding \n           src_vocab_size: vocabulary size of source\n           target_vocab_size: vocabulary size of target\n           seq_length : length of input sequence\n           num_layers: number of encoder layers\n           expansion_factor: factor which determines number of linear layers in feed forward layer\n           n_heads: number of heads in multihead attention\n        \n        \"\"\"\n        \n        self.target_vocab_size = target_vocab_size\n\n        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_trg_mask(self, trg):\n        \"\"\"\n        Args:\n            trg: target sequence\n        Returns:\n            trg_mask: target mask\n        \"\"\"\n        batch_size, trg_len = trg.shape\n        # returns the lower triangular part of matrix filled with ones\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n            batch_size, 1, trg_len, trg_len\n        )\n        return trg_mask    \n\n    def decode(self,src,trg):\n        \"\"\"\n        for inference\n        Args:\n            src: input to encoder \n            trg: input to decoder\n        out:\n            out_labels : returns final prediction of sequence\n        \"\"\"\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src)\n        out_labels = []\n        batch_size,seq_len = src.shape[0],src.shape[1]\n        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n        out = trg\n        for i in range(seq_len): #10\n            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n            # taking the last token\n            out = out[:,-1,:]\n     \n            out = out.argmax(-1)\n            out_labels.append(out.item())\n            out = torch.unsqueeze(out,axis=0)\n          \n        \n        return out_labels\n    \n    def forward(self, src, trg):\n        \"\"\"\n        Args:\n            src: input to encoder \n            trg: input to decoder\n        out:\n            out: final vector which returns probabilities of each target word\n        \"\"\"\n        trg_mask = self.make_trg_mask(trg)\n        enc_out = self.encoder(src)\n   \n        outputs = self.decoder(trg, enc_out, trg_mask)\n        return outputs\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nsrc_vocab_size = 11\ntarget_vocab_size = 11\nnum_layers = 6\nseq_length= 12\n\n\n# let 0 be sos token and 1 be eos token\nsrc = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\ntarget = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n\nprint(src.shape,target.shape)\nmodel = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n                    num_layers=num_layers, expansion_factor=4, n_heads=8)\nmodel\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}