{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":9687536,"sourceType":"datasetVersion","datasetId":5922197},{"sourceId":10051722,"sourceType":"datasetVersion","datasetId":4581967},{"sourceId":200567623,"sourceType":"kernelVersion"},{"sourceId":208026740,"sourceType":"kernelVersion"},{"sourceId":118192,"sourceType":"modelInstanceVersion","modelInstanceId":99392,"modelId":123481}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi, everyone! I'm a newcomer to this competition, and I'm here to learn as much as I can. In this notebook, I try to use LLM to generate main idea of questions at first, and use the idea as input features in the first time retrieval. In the second time retrieval, I divide the input features into two parts, and then searched for the top-25 separately. \n\nFinally, the search results are combined, and the results are sorted and output according to similarity.\n\nBut it's a pity that cv score decrease.\n\n--------------------------\n\nIf you find this helpful, please consider voting for [takanashihumbert](https://www.kaggle.com/code/takanashihumbert/eedi-qwen-2-5-32b-awq-two-time-retrieval#LLM-Reasoning) as well:\n\n* The main idea of this notebook is using retrieval two times.\n  * The first time: Get the top-K1 relavent misconceptions to LLM as a reference(using ConstructName + SubjectName).\n  * The second time: Get the top-K2(K2 < K1) relavent misconceptions(using ConstructName + SubjectName + Question + Answer + LLM's output).\n  * Inference time: ~2 hours\n  \n  \n\nThanks to these great works:\n- [Zero-shot w/ LLM feature (LB: 0.180)](https://www.kaggle.com/code/ubamba98/eedi-zero-shot-w-llm-feature-lb-0-180)\n- [Infer BGE Synthetic Data](https://www.kaggle.com/code/minhnguyendichnhat/infer-bge-synthetic-data)\n- [Fine-tuning bge Train](https://www.kaggle.com/code/sinchir0/fine-tuning-bge-train)","metadata":{}},{"cell_type":"markdown","source":"## cv and lb score\n\n\n\n**Here is the table about running result.**\n\n| Version | CV_score | LB_score |\n|------|------------|------------|\n| 1    | 0.3537187876767431 | 0.373 |\n","metadata":{}},{"cell_type":"code","source":"%%time\n# 卸载名为torch的包\n!pip uninstall -y torch\n# 命令安装名为vllm的包\n!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n# 升级名为grpcio的包\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# 升级名为ray的包\n!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n# 安装名为sentence_transformers的包\n!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:26:31.926458Z","iopub.execute_input":"2024-11-30T09:26:31.927824Z","iopub.status.idle":"2024-11-30T09:30:06.623371Z","shell.execute_reply.started":"2024-11-30T09:26:31.927783Z","shell.execute_reply":"2024-11-30T09:30:06.622348Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nCPU times: user 2.18 s, sys: 530 ms, total: 2.71 s\nWall time: 3min 34s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, math, numpy as np\nimport os\nfrom transformers import AutoTokenizer\nimport pandas as pd\nfrom tqdm import tqdm\nimport re, gc    # gc负责垃圾回收\nimport torch\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" # 指定哪些 GPU 设备对当前 Python 进程可见\npd.set_option('display.max_rows', 300)\"\"  # 控制输出行数","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-30T09:30:09.337080Z","iopub.execute_input":"2024-11-30T09:30:09.337502Z","iopub.status.idle":"2024-11-30T09:30:09.347521Z","shell.execute_reply.started":"2024-11-30T09:30:09.337467Z","shell.execute_reply":"2024-11-30T09:30:09.345929Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[9], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    pd.set_option('display.max_rows', 300)\"\"  # 控制输出行数\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (2228423496.py, line 9)","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"## Metric\n\n评价函数","metadata":{}},{"cell_type":"code","source":"%%writefile eedi_metrics.py\n# 写一个评价函数的python文件\n# 这实现的是官方的MAP@25评价\n# Credit: https://www.kaggle.com/code/abdullahmeda/eedi-map-k-metric\n\nimport numpy as np\n# Average Precision at k，简称AP@k\ndef apk(actual, predicted, k=25):\n    \"\"\"\n    Computes the average precision at k.\n    \n    This function computes the average prescision at k between two lists of\n    items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    # 没有真实值\n    if not actual:\n        return 0.0\n    # 截断使得最多只有k个预测值\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    # 计算预测量中有多少个量 匹配 实际量\n    for i,p in enumerate(predicted):\n        # first condition checks whether it is valid prediction，预测是正确的\n        # second condition checks if prediction is not repeated，预测不是重复的\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n          \n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=25):\n    \"\"\"\n    Computes the mean average precision at k.\n    \n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    \n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n        \n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.350599Z","iopub.status.idle":"2024-11-30T09:30:09.351115Z","shell.execute_reply.started":"2024-11-30T09:30:09.350927Z","shell.execute_reply":"2024-11-30T09:30:09.350944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare dataframe，改IS_SUBMISSION为0来查看cv分数\n\n数据预处理","metadata":{}},{"cell_type":"code","source":"IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")) # 判断是初次运行还是多次运行\n# 载入模型的路径，这里使用qwen2.5，阿里千问\nmodel_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n# 分词器，有待了解\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# 取100个样本当训练集，空值都填了-1\ndf_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(100, random_state=42).reset_index(drop=True)\ndf_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.352845Z","iopub.status.idle":"2024-11-30T09:30:09.353204Z","shell.execute_reply.started":"2024-11-30T09:30:09.353020Z","shell.execute_reply":"2024-11-30T09:30:09.353050Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# first retrieval\n第一次检索","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\nif not IS_SUBMISSION:\n    # 如果是第一次运行\n    df_ret = df_train.copy()\nelse:\n    # 不是第一次运行就提交测试\n    df_ret = df_test.copy()\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\nmodel = SentenceTransformer('/kaggle/input/eedi-finetuned-bge-public/Eedi-finetuned-bge')\n# model = SentenceTransformer('/kaggle/input/fine-tuning-bge-train/trained_model')\ndf_ret.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.354286Z","iopub.status.idle":"2024-11-30T09:30:09.354580Z","shell.execute_reply.started":"2024-11-30T09:30:09.354434Z","shell.execute_reply":"2024-11-30T09:30:09.354449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 这个函数的目的是清洗文本数据，使其更适合后续的文本分析或机器学习任务。\n# 通过删除特殊字符、URL、提及等，并将所有文本转换为小写，可以减少数据的噪声，使得特征提取更加准确。\ndef preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(\"@\\w+\", '',x)      # Delete strings starting with @\n    #x = re.sub(\"'\\d+\", '',x)      # Delete Numbers\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r\"\\times+\", \"\\\\\\\\times\", x)  # Replace 'times' with '\\times' with spaces around\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.355790Z","iopub.status.idle":"2024-11-30T09:30:09.356102Z","shell.execute_reply.started":"2024-11-30T09:30:09.355922Z","shell.execute_reply":"2024-11-30T09:30:09.355935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 希望能提取问题的关键词\n'''\n比如：\nQ——问题：\nConstructName：Simplify an algebraic fraction by factorising the numerator\nSubjectName：BIDMAS\nQuestionText：\"\\[3 \\times 2+4-5\\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\"\nA——答案：\nUnderstanding and applying the order of operations to achieve a specific result.\n'''\n\nfirstPROMPT  = \"\"\"\nYou are a Mathematics master. Your task is to succinctly summarize the core math knowledge from the information. \nUse your own words to convey the essence without directly referencing the problem's text, especially number in the text.\n\nInfomation：\nHere is a question about {ConstructName}({SubjectName}).\nQuestion: {Question}\n\nExample:\nInfomation：\nHere is a question about Simplify an algebraic fraction by factorising the numerator(BIDMAS).\nQuestion: \\[3 \\times 2+4-5\\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\nYour answer: Understanding and applying the order of operations to achieve a specific result.\n\"\"\"\ndef apply_keywordTemplate(row, tokenizer):\n    # 有关于聊天模板的设置\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": preprocess_text(\n                firstPROMPT.format(\n                    ConstructName=row[\"ConstructName\"],\n                    SubjectName=row[\"SubjectName\"],\n                    Question=row[\"QuestionText\"],\n                )\n            )\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\ndf_keyword = {}\nfor idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n    df_keyword[f\"{row.QuestionId}\"] = apply_keywordTemplate(row, tokenizer)\n\n\ndf_keyword = pd.DataFrame([df_keyword]).T.reset_index()\ndf_keyword.columns = [\"QuestionId\", \"text\"]    # 存储 问题id、prompt\ndf_keyword.to_parquet(\"forKeyword.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.361217Z","iopub.status.idle":"2024-11-30T09:30:09.361491Z","shell.execute_reply.started":"2024-11-30T09:30:09.361352Z","shell.execute_reply":"2024-11-30T09:30:09.361366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 输出 df_keyword DataFrame 中第一行的 text 列的值\nprint(df_keyword.loc[0, 'text'])","metadata":{"execution":{"iopub.status.busy":"2024-11-30T09:30:09.362242Z","iopub.status.idle":"2024-11-30T09:30:09.362526Z","shell.execute_reply.started":"2024-11-30T09:30:09.362382Z","shell.execute_reply":"2024-11-30T09:30:09.362395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile run_vllmKeyword.py\n# 新建大语言模型的提取关键词文件\nimport re\nimport vllm\nimport pandas as pd\n\ndf = pd.read_parquet(\"forKeyword.parquet\")\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n\nllm = vllm.LLM(\n    model_path,\n    quantization=\"awq\",          # 动态量化，即在模型推理时动态地对权重进行量化\n    tensor_parallel_size=2,      # 设置张量并行的大小，意味着模型将在两个设备上并行处理数据\n    gpu_memory_utilization=0.90, # 模型将尝试使用最多90%的可用GPU内存\n    trust_remote_code=True,      # 模型将执行来自远程源的代码\n    dtype=\"half\",                # 16位浮点数\n    enforce_eager=True,          # 急切执行（Eager Execution）是一种立即执行操作的模式\n    max_model_len=5120,          # 设置模型的最大生成长度，单位通常是令牌数\n    disable_log_stats=True       # 禁用日志\n)\n# 获取模型的分词器（tokenizer）\ntokenizer = llm.get_tokenizer()\n\n\nresponses = llm.generate(\n    df[\"text\"].values,  # 传入prompt\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.每个提示返回的输出序列数量\n        top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.顶部令牌的累积概率\n        temperature=0.1,  # randomness of the sampling.控制采样的随机性\n        seed=777, # Seed for reprodicibility.随机种子\n        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n        max_tokens=1024,  # Maximum number of tokens to generate per output sequence.\n    ),\n    use_tqdm=True\n)\n\nresponses = [x.outputs[0].text for x in responses]\n# 新增一列\ndf[\"fullLLMText\"] = responses\n\ndef extract_response(text):   # 没用上\n    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n# 新增一列，代表llm给出的误差项\ndf[\"llmKeyword\"] = responses\ndf.to_parquet(\"forKeyword.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.77376Z","iopub.execute_input":"2024-11-14T09:13:56.774122Z","iopub.status.idle":"2024-11-14T09:13:56.782565Z","shell.execute_reply.started":"2024-11-14T09:13:56.774072Z","shell.execute_reply":"2024-11-14T09:13:56.781497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run_vllmKeyword.py","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.786582Z","iopub.execute_input":"2024-11-14T09:13:56.786927Z","iopub.status.idle":"2024-11-14T09:13:56.792622Z","shell.execute_reply.started":"2024-11-14T09:13:56.786893Z","shell.execute_reply":"2024-11-14T09:13:56.791623Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm_output = pd.read_parquet(\"forKeyword.parquet\")\n\nfor idx, row in llm_output[0:5].iterrows():\n    print(row.llmKeyword)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.793896Z","iopub.execute_input":"2024-11-14T09:13:56.7943Z","iopub.status.idle":"2024-11-14T09:13:56.801102Z","shell.execute_reply.started":"2024-11-14T09:13:56.794251Z","shell.execute_reply":"2024-11-14T09:13:56.80016Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, row in llm_output[0:5].iterrows():\n    print(row.text)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.802339Z","iopub.execute_input":"2024-11-14T09:13:56.802681Z","iopub.status.idle":"2024-11-14T09:13:56.809021Z","shell.execute_reply.started":"2024-11-14T09:13:56.802629Z","shell.execute_reply":"2024-11-14T09:13:56.808171Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_keyword(row):\n    flag = 0\n    if \"your answer:\" in row['llmKeyword']:\n        flag = 1\n    text = row['llmKeyword'].strip()\n    # 假设text 为 \"your answer: mastering the subtraction of decimals with varying decimal places to achieve accurate results.\"\n    if flag:\n        # 删去text中的\"your answer:\"\n        text = text.replace(\"your answer:\", \"\").strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.810165Z","iopub.execute_input":"2024-11-14T09:13:56.810478Z","iopub.status.idle":"2024-11-14T09:13:56.818397Z","shell.execute_reply.started":"2024-11-14T09:13:56.810445Z","shell.execute_reply":"2024-11-14T09:13:56.817358Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm_output['llmKeywordCleaned'] = llm_output.apply(clean_keyword, axis=1)\nfor idx, row in llm_output[0:5].iterrows():\n    print(row.llmKeywordCleaned)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.819669Z","iopub.execute_input":"2024-11-14T09:13:56.820024Z","iopub.status.idle":"2024-11-14T09:13:56.826935Z","shell.execute_reply.started":"2024-11-14T09:13:56.819983Z","shell.execute_reply":"2024-11-14T09:13:56.826024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 按列拼接，之后清理\ndf_ret['input_features'] = df_ret[\"ConstructName\"] + \". \" + df_ret[\"SubjectName\"] + \". \" + llm_output['llmKeywordCleaned']\ndf_ret['input_features'] = df_ret['input_features'].apply(lambda x: preprocess_text(x))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.828109Z","iopub.execute_input":"2024-11-14T09:13:56.828418Z","iopub.status.idle":"2024-11-14T09:13:56.840202Z","shell.execute_reply.started":"2024-11-14T09:13:56.828384Z","shell.execute_reply":"2024-11-14T09:13:56.839273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 对输入特征进行编码\nembedding_query = model.encode(df_ret['input_features'], convert_to_tensor=True)\n# 返回numpy数组，存储MisconceptionName的values\nmisconceptions = df_misconception_mapping.MisconceptionName.values\n# 对误差项进行编码\nembedding_Misconception = model.encode(misconceptions, convert_to_tensor=True)\n\n# the first time retrieval for LLM prompt\n# 简单理解，就是抛出输入特征，找到前k个比较符合输入特征方向的误差项解释\n# 它计算 embedding_query 和 embedding_Misconception 中每个嵌入之间的相似度。\n# 根据相似度分数，它对所有误区进行排序。\n# 它返回相似度最高的前100个误区的ID。\nRet_topNids = util.semantic_search(embedding_query, embedding_Misconception, top_k=100)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:13:56.841326Z","iopub.execute_input":"2024-11-14T09:13:56.841696Z","iopub.status.idle":"2024-11-14T09:14:08.751987Z","shell.execute_reply.started":"2024-11-14T09:13:56.841654Z","shell.execute_reply":"2024-11-14T09:14:08.750812Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retrivals = []    # 用于存储每个问题的检索结果\ndicts = {}        # 存储每个问题的检索误区的详细信息\nfor idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n    top_ids = Ret_topNids[idx]\n    retrival = ''                       # 对于每一行，从 Ret_topNids 列表中获取对应的最相关的误区ID列表\n    dicts[str(row['QuestionId'])] = {}  # 用于构建当前问题的检索结果\n    for i, ids in enumerate(top_ids):\n        # serial number + misconceptions\n        retrival += f'{i+1}. ' + misconceptions[ids['corpus_id']] + '\\n'\n        # save retrieved misconceptions for each QuestionId.\n        dicts[str(row['QuestionId'])][str(i+1)] = misconceptions[ids['corpus_id']]\n    retrivals.append(retrival)\n\n# 把检索加到了后面\ndf_ret['Retrival'] = retrivals\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:14:08.886311Z","iopub.execute_input":"2024-11-14T09:14:08.887027Z","iopub.status.idle":"2024-11-14T09:14:09.019282Z","shell.execute_reply.started":"2024-11-14T09:14:08.886978Z","shell.execute_reply":"2024-11-14T09:14:09.01817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(x):\n    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = re.sub(r\"\\times+\", \"\\\\\\\\times\", x)  # Replace 'times' with '\\times' \n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\nPROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\nQuestion: {Question}\nCorrect Answer: {CorrectAnswer}\nIncorrect Answer: {IncorrectAnswer}\n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the incorrect answer.\nNo need to give the reasoning process and do not use \"The misconception is\" to start your answers.\nThere are some relative and possible misconceptions below to help you make the decision:\n\n{Retrival}\n\"\"\"\n# just directly give your answers.\n# 行（对应df_ret中每一行），向量化文本类，目标列（对应不正确的答案）\ndef apply_template(row, tokenizer, targetCol):\n    # 有关于聊天模板的设置\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": preprocess_text(\n                PROMPT.format(\n                    ConstructName=row[\"ConstructName\"],\n                    SubjectName=row[\"SubjectName\"],\n                    Question=row[\"QuestionText\"],\n                    IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n                    CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"],\n                    Retrival=row[f\"Retrival\"]\n                )\n            )\n        }\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return text\n\ndf = {}\nif not IS_SUBMISSION:\n    # 如果是第一次运行\n    df_label = {}\n    for idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            # 如果当前option不是正确答案且当前option有误解项\n            if (row.CorrectAnswer!=option) & (row[f\"Misconception{option}Id\"]!=-1):\n                # 生成模板并存储\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n                df_label[f\"{row.QuestionId}_{option}\"] = [row[f\"Misconception{option}Id\"]]  # 提取真实值的\n    # 将 df_label 字典转换为 DataFrame，并进行转置（.T），使得每个键值对成为一行，然后重置索引            \n    df_label = pd.DataFrame([df_label]).T.reset_index()\n    df_label.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]  # 设置列名为 QuestionId_Answer 和 MisconceptionId\n    df_label.to_parquet(\"label.parquet\", index=False)\nelse:\n    for idx, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n        for option in [\"A\", \"B\", \"C\", \"D\"]:\n            # 如果当前option不是正确答案\n            if row.CorrectAnswer!=option:\n                df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n\ndf = pd.DataFrame([df]).T.reset_index()\ndf.columns = [\"QuestionId_Answer\", \"text\"]    # 存储 问题id_错误答案、prompt\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:14:09.020823Z","iopub.execute_input":"2024-11-14T09:14:09.02116Z","iopub.status.idle":"2024-11-14T09:14:09.348341Z","shell.execute_reply.started":"2024-11-14T09:14:09.021123Z","shell.execute_reply":"2024-11-14T09:14:09.347363Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 输出 df DataFrame 中第一行的 text 列的值\nprint(df.loc[0, 'text'])","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:14:09.349709Z","iopub.execute_input":"2024-11-14T09:14:09.350019Z","iopub.status.idle":"2024-11-14T09:14:09.35523Z","shell.execute_reply.started":"2024-11-14T09:14:09.349985Z","shell.execute_reply":"2024-11-14T09:14:09.354227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LLM Reasoning\n运用大语言模型推理","metadata":{}},{"cell_type":"code","source":"%%writefile run_vllm.py\n\nimport re\nimport vllm\nimport pandas as pd\n\ndf = pd.read_parquet(\"submission.parquet\")\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n\nllm = vllm.LLM(\n    model_path,\n    quantization=\"awq\",          # 动态量化，即在模型推理时动态地对权重进行量化\n    tensor_parallel_size=2,      # 设置张量并行的大小，意味着模型将在两个设备上并行处理数据\n    gpu_memory_utilization=0.90, # 模型将尝试使用最多90%的可用GPU内存\n    trust_remote_code=True,      # 模型将执行来自远程源的代码\n    dtype=\"half\",                # 16位浮点数\n    enforce_eager=True,          # 急切执行（Eager Execution）是一种立即执行操作的模式\n    max_model_len=5120,          # 设置模型的最大生成长度，单位通常是令牌数\n    disable_log_stats=True       # 禁用日志\n)\n# 获取模型的分词器（tokenizer）\ntokenizer = llm.get_tokenizer()\n\n\nresponses = llm.generate(\n    df[\"text\"].values,  # 传入prompt\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.每个提示返回的输出序列数量\n        top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.顶部令牌的累积概率\n        temperature=0.1,  # randomness of the sampling.控制采样的随机性\n        seed=777, # Seed for reprodicibility.随机种子\n        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n        max_tokens=1024,  # Maximum number of tokens to generate per output sequence.\n    ),\n    use_tqdm=True\n)\n\nresponses = [x.outputs[0].text for x in responses]\n# 新增一列\ndf[\"fullLLMText\"] = responses\n\ndef extract_response(text):   # 没用上\n    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n# 新增一列，代表llm给出的误差项\ndf[\"llmMisconception\"] = responses\ndf.to_parquet(\"submission.parquet\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:14:09.356504Z","iopub.execute_input":"2024-11-14T09:14:09.35684Z","iopub.status.idle":"2024-11-14T09:14:09.36505Z","shell.execute_reply.started":"2024-11-14T09:14:09.356805Z","shell.execute_reply":"2024-11-14T09:14:09.364101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run_vllm.py","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:14:09.366128Z","iopub.execute_input":"2024-11-14T09:14:09.366403Z","iopub.status.idle":"2024-11-14T09:32:21.339516Z","shell.execute_reply.started":"2024-11-14T09:14:09.366374Z","shell.execute_reply":"2024-11-14T09:32:21.338147Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"llm_output = pd.read_parquet(\"submission.parquet\")\n\nfor idx, row in llm_output[0:5].iterrows():\n    print(row.llmMisconception)\n    print(\"===\"*6)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:21.34164Z","iopub.execute_input":"2024-11-14T09:32:21.342107Z","iopub.status.idle":"2024-11-14T09:32:21.409587Z","shell.execute_reply.started":"2024-11-14T09:32:21.342058Z","shell.execute_reply":"2024-11-14T09:32:21.408622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"```\n1\n==================\nWhen dividing decimals with the same number of decimal places as each other, assumes the answer also has the same number of decimal places.\n==================\n11. Thinks index notation represents repeated addition rather than repeated multiplication\n==================\nMixes up squaring and multiplying by 2 or doubling\n==================\n11. When rounding to decimal places and the only digits after the decimal point will be zeros, just gives the integer as the answer.\n==================\n```","metadata":{}},{"cell_type":"code","source":"text = llm_output.loc[0, 'text']\nPREFIX = \"<|im_start|>user\"\ntext = text.split(PREFIX)[1].split(\"You are a Mathematics teacher.\")[0].strip('\\n').split('Here is a question about')[-1].strip()\nprint(text)\n# 提取问题","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:21.410778Z","iopub.execute_input":"2024-11-14T09:32:21.411137Z","iopub.status.idle":"2024-11-14T09:32:21.417892Z","shell.execute_reply.started":"2024-11-14T09:32:21.411089Z","shell.execute_reply":"2024-11-14T09:32:21.416725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Post-processing for LLM output\n对生成的输出进行后续处理","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\n\ndf = pd.read_parquet(\"submission.parquet\")\ndf_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n\nmodel = SentenceTransformer('/kaggle/input/eedi-finetuned-bge-public/Eedi-finetuned-bge')","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:21.419278Z","iopub.execute_input":"2024-11-14T09:32:21.419718Z","iopub.status.idle":"2024-11-14T09:32:31.867162Z","shell.execute_reply.started":"2024-11-14T09:32:21.419673Z","shell.execute_reply":"2024-11-14T09:32:31.866264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def number2sentence(row):\n    \"\"\"\n    This is used for post-processing of LLM's output.\n    Since we give top-N retrieval to the LLM with serial number,\n    Sometimes the LLM will only output the serial number without any sentence.\n    We use the 'dicts' generated at the beginning to map the serial number with corresponding misconceptions.\n    LLM有时会只输出序列号而不输出句子，使用之前生成的dicts来映射序列号和对应的误区\n    \"\"\"\n    text = row['llmMisconception'].strip()\n    # potential is the most possible serial number in LLM output.\n    potential = re.search(r'^\\w+\\.{0,1}', text).group()  # 选取text开头的语或词\n    if '.' in potential:\n        sentence = text.replace(potential, '').strip()   # 删去序号和小数点\n    # if the LLM output is only a serial number, we map it with corresponding misconceptions saved in the dict.\n    elif len(potential) == len(text):\n        # 将这个问题的前N个可能的误解项提出来，以便之后映射\n        qid_retrieval = dicts[row['QuestionId']]\n        try:\n            # qid_retrieval is the top-N misconceptions for an QuestionId,\n            # qid_retrieval[potential] is the most possible misconception.\n            sentence = qid_retrieval[potential]\n        except:\n            # If the mapping fails, we use the first one(the most possible one in the first retrieval).\n            sentence = qid_retrieval['1']\n    else:\n        sentence = text\n        \n    return sentence\n\n\ndf['QuestionId'] = df['QuestionId_Answer'].apply(lambda x: x.split('_')[0])\n# 进行误差项输出的处理\ndf['llmMisconception_clean'] = df.apply(number2sentence, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.868602Z","iopub.execute_input":"2024-11-14T09:32:31.869029Z","iopub.status.idle":"2024-11-14T09:32:31.88576Z","shell.execute_reply.started":"2024-11-14T09:32:31.86898Z","shell.execute_reply":"2024-11-14T09:32:31.884832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.886915Z","iopub.execute_input":"2024-11-14T09:32:31.887231Z","iopub.status.idle":"2024-11-14T09:32:31.9076Z","shell.execute_reply.started":"2024-11-14T09:32:31.887201Z","shell.execute_reply":"2024-11-14T09:32:31.906447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Second retrieval","metadata":{}},{"cell_type":"code","source":"def preprocess_text(x):\n    x = x.lower()                 # Convert words to lowercase\n    x = re.sub(r\"@\\w+\", '',x)      # Delete strings starting with @\n    #x = re.sub(r\"\\d+\", '',x)      # Delete Numbers\n    x = re.sub(r\"http\\w+\", '',x)   # Delete URL\n    x = re.sub(r\"\\\\\\(\", \" \", x)\n    x = re.sub(r\"\\\\\\)\", \" \", x)\n    x = re.sub(r\"[ ]{1,}\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\times+\", \"\\\\\\\\times\", x)  # Replace 'times' with '\\times' with spaces around\n    x = x.strip()                 # Remove empty characters at the beginning and end\n    return x\n\nPREFIX = \"<|im_start|>user\"\ndf['input_features'] = df[\"text\"].apply(lambda x: x.split(PREFIX)[1].split(\"You are a Mathematics teacher.\")[0].strip('\\n').split('Here is a question about')[-1].strip())\n\ndf['input_features'] = df['input_features'].apply(lambda x: preprocess_text(x))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.91389Z","iopub.execute_input":"2024-11-14T09:32:31.91439Z","iopub.status.idle":"2024-11-14T09:32:31.93845Z","shell.execute_reply.started":"2024-11-14T09:32:31.914356Z","shell.execute_reply":"2024-11-14T09:32:31.93744Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['input_features']\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.939813Z","iopub.execute_input":"2024-11-14T09:32:31.940142Z","iopub.status.idle":"2024-11-14T09:32:31.955977Z","shell.execute_reply.started":"2024-11-14T09:32:31.94011Z","shell.execute_reply":"2024-11-14T09:32:31.954918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"llmMisconception_clean\"]","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.95735Z","iopub.execute_input":"2024-11-14T09:32:31.95779Z","iopub.status.idle":"2024-11-14T09:32:31.969748Z","shell.execute_reply.started":"2024-11-14T09:32:31.957752Z","shell.execute_reply":"2024-11-14T09:32:31.968825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mixture\nembedding_query = model.encode(df['input_features'], convert_to_tensor=True)\nembedding_Misconception = model.encode(df_misconception_mapping.MisconceptionName.values, convert_to_tensor=True)\ntop25ids1 = util.semantic_search(embedding_query, embedding_Misconception, top_k=25)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:31.971205Z","iopub.execute_input":"2024-11-14T09:32:31.971642Z","iopub.status.idle":"2024-11-14T09:32:49.417483Z","shell.execute_reply.started":"2024-11-14T09:32:31.971585Z","shell.execute_reply":"2024-11-14T09:32:49.416608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(top25ids1))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:49.419022Z","iopub.execute_input":"2024-11-14T09:32:49.419496Z","iopub.status.idle":"2024-11-14T09:32:49.425283Z","shell.execute_reply.started":"2024-11-14T09:32:49.419417Z","shell.execute_reply":"2024-11-14T09:32:49.424221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nembedding_query = model.encode(df[\"llmMisconception_clean\"], convert_to_tensor=True)\nembedding_Misconception = model.encode(df_misconception_mapping.MisconceptionName.values, convert_to_tensor=True)\ntop25ids2 = util.semantic_search(embedding_query, embedding_Misconception, top_k=25)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:32:49.426683Z","iopub.execute_input":"2024-11-14T09:32:49.427001Z","iopub.status.idle":"2024-11-14T09:33:02.947265Z","shell.execute_reply.started":"2024-11-14T09:32:49.426967Z","shell.execute_reply":"2024-11-14T09:33:02.946468Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top25ids = []\nfor i in range(len(top25ids1)):\n    top25id = top25ids1[i] + top25ids2[i]\n    top25ids.append(top25id)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:02.948595Z","iopub.execute_input":"2024-11-14T09:33:02.949039Z","iopub.status.idle":"2024-11-14T09:33:02.954873Z","shell.execute_reply.started":"2024-11-14T09:33:02.94899Z","shell.execute_reply":"2024-11-14T09:33:02.953715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(top25ids)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:02.956135Z","iopub.execute_input":"2024-11-14T09:33:02.956497Z","iopub.status.idle":"2024-11-14T09:33:02.966693Z","shell.execute_reply.started":"2024-11-14T09:33:02.956445Z","shell.execute_reply":"2024-11-14T09:33:02.965705Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 筛选分数\nnew_top25ids = []\nfor top25id in top25ids:\n    top25idss = sorted(top25id, key=lambda x: x['score'], reverse=True)\n    new_top25id = []\n    for item in top25idss :\n        if len(new_top25id) >= 25:\n            break\n        if item['score'] >= 0.6:\n            new_top25id.append(item)\n        else:\n            break\n    new_top25ids.append(new_top25id)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:02.967771Z","iopub.execute_input":"2024-11-14T09:33:02.968452Z","iopub.status.idle":"2024-11-14T09:33:02.981491Z","shell.execute_reply.started":"2024-11-14T09:33:02.968393Z","shell.execute_reply":"2024-11-14T09:33:02.980631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(new_top25ids)","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:02.982604Z","iopub.execute_input":"2024-11-14T09:33:02.983282Z","iopub.status.idle":"2024-11-14T09:33:02.991327Z","shell.execute_reply.started":"2024-11-14T09:33:02.983216Z","shell.execute_reply":"2024-11-14T09:33:02.990232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"MisconceptionId\"] = [\" \".join([str(x[\"corpus_id\"]) for x in top25id]) for top25id in new_top25ids]\n# 输出提交文件\ndf[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:02.992587Z","iopub.execute_input":"2024-11-14T09:33:02.992922Z","iopub.status.idle":"2024-11-14T09:33:03.023732Z","shell.execute_reply.started":"2024-11-14T09:33:02.992891Z","shell.execute_reply":"2024-11-14T09:33:03.022789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sanity","metadata":{}},{"cell_type":"code","source":"# 计算误差\nif not IS_SUBMISSION:\n    import pandas as pd\n    from eedi_metrics import mapk\n    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n    label = pd.read_parquet(\"label.parquet\")[\"MisconceptionId\"]\n    print(\"Validation: \", mapk(label, predicted))","metadata":{"execution":{"iopub.status.busy":"2024-11-14T09:33:03.02518Z","iopub.execute_input":"2024-11-14T09:33:03.025607Z","iopub.status.idle":"2024-11-14T09:33:03.08789Z","shell.execute_reply.started":"2024-11-14T09:33:03.02555Z","shell.execute_reply":"2024-11-14T09:33:03.086897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}