# -*- coding: utf-8 -*-
"""late-chunking-cross-lingual-jina-v3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iz3ACFs5aLV2O_uZEjiR1aHGlXqj0HY7

# Late Chunking & Cross-Lingual Retrieval with `jina-embeddings-v3`

We start by downloading the first chapter of *Alice in Wonderland* in multiple languages The downloaded files contain translations of in five different languages:

- **English** (`aaw-en.txt`)
- **German** (`aaw-de.txt`)
- **French** (`aaw-fr.txt`)
- **Japanese** (`aaw-jp.txt`)
- **Chinese** (`aaw-zh.txt`)
"""

# Download the full book of Alice in Wonderland
# !wget -q https://gist.githubusercontent.com/hanxiao/4a4459601ce5595ec4dd57a4a70504bd/raw/27308f9c93ffac3384ede344accf8c5a491b71bf/aaw-{en,de,fr,jp,zh}.txt

# Download first chapter only of Alice in Wonderland
!wget -q https://gist.githubusercontent.com/hanxiao/4a4459601ce5595ec4dd57a4a70504bd/raw/b9b2817172f53b5e1a452990fd445dba773a88f1/aaw-{en,de,fr,jp,zh}.txt

"""### Load Jina Model and Set Task for Text Matching

This step loads a pre-trained embedding model and tokenizer from the specified model name (`jinaai/jina-embeddings-v3`). A task ID specifies to use the "text-matching" adapter and creates an adaptation map used later for inference.
"""

import torch
from transformers import AutoModel, AutoTokenizer

MODEL_NAME = 'jinaai/jina-embeddings-v3'
DEVICE='cuda'
NGRAM=10

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
task_id = model._adaptation_map['text-matching']
adapter_mask = torch.full((1,), task_id, dtype=torch.int32, device=model.device)
model.eval()
model = model.to(DEVICE)

"""### Load Alice in Wonderland Text Files by Language

This step initializes a dictionary `filenames` containing language codes as keys and corresponding file names as values. It then creates an empty dictionary, `alice_texts`, to store the text contents. For each language, it opens the associated file, reads the text, and stores it in `alice_texts` under the respective language code.
"""

filenames = {
    'en': 'aaw-en.txt',
    'de': 'aaw-de.txt',
    'fr': 'aaw-fr.txt',
    'jp': 'aaw-jp.txt',
    'zh': 'aaw-zh.txt',
}
alice_texts = dict()
for lang, fname in filenames.items():
  with open(fname, 'r') as f:
    alice_texts[lang] = f.read()

"""### Encoding the texts with Late Chunking

For each language in `alice_texts`, we tokenize the text, encode it with Jina embeddings, and retrieve token-level embeddings. Using a sliding window approach, we compute mean embeddings over windows of varying sizes (`n = 1` to `9`) by calling the `sliding_window_mean` function. This function calculates the average embeddings within each window and stores them in `sliding_window_matrices`. Additionally, `sliding_window_spans` captures the start and end character positions for each window span, enabling us to map averaged embeddings back to the original text positions.

"""

import numpy as np

def embed_with_overlap(model, model_inputs, macro_chunk_size=2048, overlap_size=256):
    """ Implements Long-Late-Chunking for encoding long text sequences and avoid
    """
    len_tokens = len(model_inputs["input_ids"][0])

    if len_tokens > macro_chunk_size:
        indices = []
        for i in range(
            0,
            len_tokens,
            macro_chunk_size
            - overlap_size,
        ):
            start = i
            end = min(i + macro_chunk_size, len_tokens)
            indices.append((start, end))
    else:
        indices = [(0, len_tokens)]

    outputs = []
    for start, end in indices:
        batch_inputs = {k: v[:, start:end] for k, v in model_inputs.items()}

        with torch.no_grad():
            model_output = model(**batch_inputs)

        if start > 0:
            outputs.append(
                model_output[0][:, overlap_size :]
            )
        else:
            outputs.append(model_output[0])

    return torch.cat(outputs, dim=1).to(model.device)


def sliding_window_mean(matrix, window_size):
    # Ensure the window size is valid
    if window_size < 1 or window_size > matrix.shape[0]:
        raise ValueError("Window size must be between 1 and the number of rows in the matrix.")

    # Initialize an empty list to store the mean values for each window position
    smoothed_matrix = []

    # Apply the sliding window mean across the rows
    for i in range(matrix.shape[0] - window_size + 1):
        # Select the window of rows
        window = matrix[i:i + window_size, :]

        # Compute the mean of the window and append to the smoothed matrix
        smoothed_matrix.append(np.mean(window, axis=0))

    # Convert the list of means to a numpy array
    return np.array(smoothed_matrix)

sliding_window_matrices = dict()
sliding_window_spans = dict()

for lang, text in alice_texts.items():
  # Tokenize the Text
  tokenization = tokenizer(text, return_tensors='pt', return_offsets_mapping=True).to(DEVICE)

  # Encode the Text with Jina-Ebmeddings-V3
  with torch.no_grad():
    outputs = model(**tokenization, adapter_mask=adapter_mask) # TODO handle task
  token_embs = outputs['last_hidden_state'][0].to(dtype=torch.float32, device='cpu').detach().numpy()

  # Determine for each embedding the respective token
  token_spans = [(mapping[0], mapping[1]) for mapping in tokenization['offset_mapping'][0]]

  # Calculate late chunking representations of Sliding Windows
  sliding_window_matrices[lang] = dict()
  sliding_window_spans[lang] = dict()
  for n in range(1, NGRAM+1):
    sliding_window_matrices[lang][n] = sliding_window_mean(token_embs, n)
    sliding_window_spans[lang][n] = [(token_spans[i][0], token_spans[i+(n-1)][1]) for i in range(len(token_spans) - n + 1)]

"""### Retrieve the Most Relevant Spans

Now, we calculate the query embedding, and determine the number of tokens of the query (excluding special tokens)

Then, we define the function to retrieve the Top-K spans.
"""

import numpy as np

def find_top_k_similar_spans(vector, matrix, k=5):
    # Normalize the vector
    vector_norm = vector / np.linalg.norm(vector)

    # Normalize each row in the matrix
    matrix_norm = matrix / np.linalg.norm(matrix, axis=1, keepdims=True)

    # Compute cosine similarities
    cosine_similarities = np.dot(matrix_norm, vector_norm)

    # Get the indices of the top-k highest cosine similarities
    top_k_indices = np.argpartition(-cosine_similarities, k)[:k]

    # Sort the top-k indices by cosine similarity in descending order
    top_k_indices = top_k_indices[np.argsort(-cosine_similarities[top_k_indices])]

    return top_k_indices

"""And the function to get query embedding:"""

def get_query_embedding(query):
    token_len = tokenizer(query, return_tensors='pt')['input_ids'].shape[1]
    query_n = min(token_len-2, NGRAM)  # subtract 2 for [CLS] and [SEP] token
    query_emb = model.encode(query, task='text-matching')
    print('Number of query tokens:', query_n)
    return query_emb, query_n

import pandas as pd
from IPython.display import display, HTML, clear_output
import ipywidgets as widgets
import matplotlib.pyplot as plt
import io
import base64

def create_spike_plot(start, end, total_length, height=10, width=20):
    """Create a small spike plot showing position in text"""
    # Convert tensors to CPU if they're on GPU
    if torch.is_tensor(start):
        start = start.cpu().item()
    if torch.is_tensor(end):
        end = end.cpu().item()
    if torch.is_tensor(total_length):
        total_length = total_length.cpu().item()

    fig, ax = plt.subplots(figsize=(2, 0.3))

    # Normalize positions to 0-1 range
    start_norm = start / total_length
    end_norm = end / total_length

    # Create the spike plot
    x = [start_norm, start_norm, end_norm, end_norm]
    y = [0, 1, 1, 0]

    # Add x-axis line
    ax.axhline(y=0, color='cyan', linewidth=1)

    # Plot the spike
    ax.plot(x, y, color='cyan', linewidth=1)
    ax.set_ylim(-0.1, 1.1)
    ax.set_xlim(-0.02, 1.02)

    # Remove axes and grid
    ax.axis('off')

    # Convert plot to base64 image
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', transparent=True)
    plt.close()
    buf.seek(0)
    return f'<img src="data:image/png;base64,{base64.b64encode(buf.getvalue()).decode()}">'


def search_texts(button):
    """Function to perform the search and display results"""
    query = search_box.value
    top_k = top_k_dropdown.value

    clear_output(wait=True)
    display(widgets.HBox([search_box, top_k_dropdown, search_button]))

    query_emb, query_n = get_query_embedding(query)

    # Create results table with proper columns
    results = []

    # Get text lengths for each language
    text_lengths = {lang: len(alice_texts[lang]) for lang in alice_texts.keys()}

    for lang in alice_texts.keys():
        row_parts = [lang]  # Start with language

        result_index_ids = find_top_k_similar_spans(query_emb, sliding_window_matrices[lang][query_n], top_k)

        for rank, result_id in enumerate(result_index_ids):
            span = sliding_window_spans[lang][query_n][result_id]
            text = alice_texts[lang][span[0]:span[1]]
            plot = create_spike_plot(span[0], span[1], text_lengths[lang])

            row_parts.extend([text, plot])

        results.append(row_parts)

    # Create the table HTML manually
    table_html = f"""
    <table>
        <tr>
            <th>Language</th>
            {''.join(f'''
            <th>Rank {i+1}</th>
            <th>Position</th>
            ''' for i in range(top_k))}
        </tr>
        {''.join(f'''
        <tr>
            {''.join(f'<td style="border: 1px solid #ddd; padding: 8px; text-align: left;">{cell}</td>' for cell in row)}
        </tr>
        ''' for row in results)}
    </table>
    """

    # Display with styling
    display(HTML(f"""
    <style>
        table {{
            border-collapse: collapse;
            width: auto;
            table-layout: auto;
            margin: 0;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }}
        td {{
            vertical-align: middle;
        }}
        td:nth-child(3n) {{
            width: 100px;
            min-width: 100px;
            max-width: 100px;
        }}
        img {{
            display: block;
            width: 100px;
            height: auto;
            margin: 0 auto;
        }}
    </style>
    {table_html}
    """))

"""Finally, we apply our search function to the token indices for all languages"""

# Create the search box widget
search_box = widgets.Text(
    value='Alice open the door and see',
    placeholder='Enter your search query',
    description='Search:',
    layout=widgets.Layout(width='400px')
)

# Create a dropdown for top_k selection
top_k_dropdown = widgets.Dropdown(
    options=[1, 2, 3, 4, 5],
    value=3,
    description='Top K:',
    layout=widgets.Layout(width='150px')
)

# Create a search button
search_button = widgets.Button(
    description='Search',
    layout=widgets.Layout(width='100px')
)

# Connect the button click to the search function
search_button.on_click(search_texts)
# search_button.on_click(search_texts2)

# Display the search box, dropdown, and button side by side
# display(widgets.HBox([search_box, search_button]))
display(widgets.HBox([search_box, top_k_dropdown, search_button]))